# === Arborescence du dossier ===

hash_tool
├── .github
│   └── workflows
│       └── ci.yml
├── docker
│   └── entrypoint.sh
├── mon_dossier
│   ├── _data-destination
│   │   ├── lorem-ipsum-01-modif.txt
│   │   ├── lorem-ipsum-02.txt
│   │   ├── lorem-ipsum-03.txt
│   │   └── lorem-ipsum-04.txt
│   ├── _data-source
│   │   ├── lorem-ipsum-01-modif.txt
│   │   ├── lorem-ipsum-02.txt
│   │   ├── lorem-ipsum-03.txt
│   │   └── lorem-ipsum-04.txt
│   ├── bases
│   │   ├── hashes_destination.b3
│   │   ├── hashes_destination.b3.meta.json
│   │   ├── hashes_source.b3
│   │   └── hashes_source.b3.meta.json
│   └── result
│       └── resultats_hashes_source
│           ├── disparus.txt
│           ├── modifies.b3
│           ├── nouveaux.txt
│           ├── recap.txt
│           └── report.html
├── pipelines
│   ├── pipeline-amelioree.json
│   ├── pipeline-debug-deux-adresses.json
│   ├── pipeline-debug.json
│   ├── pipeline-veracrypt.json
│   └── pipeline.json
├── reports
│   └── template.html
├── src
│   ├── lib
│   │   ├── core.sh
│   │   ├── report.sh
│   │   ├── results.sh
│   │   └── ui.sh
│   └── integrity.sh
├── tests
│   ├── run_tests.sh
│   ├── run_tests_core.sh
│   └── run_tests_pipeline.sh
├── .dockerignore
├── .gitignore
├── Dockerfile
├── README.md
├── docker-compose.yml
├── hash-tool
├── mkdocs.yml
└── runner.sh


# === Contenu des fichiers ===

--- Fichier : .dockerignore ---
# .dockerignore - hash_tool
#
# Exclut du contexte de build Docker ce qui n'est pas nécessaire.
# Réduit la taille du contexte envoyé au daemon.

# Données utilisateur - jamais dans l'image
mon_dossier/
*.b3

# Résultats
resultats/
~/integrity_resultats/

# Tests - non requis dans l'image de production
tests/

# Documentation - non requise dans l'image
docs/
autre/
reports/
*.md
!README.md

# Fichiers temporaires
temp.txt
*.tmp
*.log

# Outils de développement
.git/
.gitignore


--- Fichier : .gitignore ---
# == Données utilisateur =======================================================
*.b3

# == Résultats =================================================================
resultats/
integrity_resultats/

# == Docker ====================================================================
# Ne pas ignorer : Dockerfile, .dockerignore, docker-compose.yml, docker/
# Ignorer les artefacts de build local
.docker/

# == Fichiers temporaires ======================================================
# Exclure les fichiers contenant le mot "temp" n'importe où dans le nom
*temp*

*.tmp
*.log
*.bak

# == OS ========================================================================
.DS_Store
Thumbs.db
desktop.ini

# == Éditeurs =================================================================
.vscode/
.idea/
*.swp
*.swo
*~

# == doc perso =================================================================
hors_git/

# == fichiers utils de dev =================================================================
.concat_config.json
gentxt.txt

# == doc généree =================================================================
site/


--- Fichier : Dockerfile ---
# =============================================================================
# hash_tool - Dockerfile
#
# Image Alpine légère (~15 Mo) avec b3sum et jq.
# Supporte linux/amd64 et linux/arm64 (NAS Synology, Raspberry Pi, etc.)
#
# b3sum est installé depuis les packages Alpine (community) - plus fiable
# que le téléchargement manuel depuis GitHub Releases.
#
# Build :
#   docker build -t hash_tool .
#   docker build --platform linux/arm64 -t hash_tool:arm64 .
#
# Utilisation :
#   docker run --rm -v /mes/donnees:/data hash_tool verify /data/base.b3
#   docker run --rm -v /mes/donnees:/data -v /mes/bases:/bases hash_tool compute /data /bases/hashes.b3
#   docker run --rm -v /chemin/pipeline.json:/pipelines/pipeline.json \
#              -v /mes/donnees:/data -v /mes/bases:/bases -v /mes/resultats:/resultats \
#              hash_tool runner /pipelines/pipeline.json
# =============================================================================

FROM alpine:3.19

LABEL maintainer="hash_tool" \
      description="Vérification d'intégrité BLAKE3 - integrity.sh + runner.sh" \
      org.opencontainers.image.source="https://github.com/hash_tool"

# Toutes les dépendances depuis apk - pas de wget, pas de binaire externe
# b3sum est dans Alpine community depuis v3.15
RUN apk add --no-cache \
      bash \
      jq \
      b3sum \
      coreutils \
      findutils \
    && rm -rf /var/cache/apk/*

# == Copie des scripts ========================================================

WORKDIR /app

COPY runner.sh           ./runner.sh
COPY src/integrity.sh    ./src/integrity.sh
COPY src/lib/report.sh   ./src/lib/report.sh

RUN chmod +x runner.sh src/integrity.sh src/lib/report.sh

# == Entrypoint ===============================================================

COPY docker/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# == Volumes ==================================================================
#
# /data       → données à hacher (montage en lecture seule recommandé)
# /bases      → fichiers .b3 (lecture/écriture)
# /pipelines  → fichiers pipeline.json
# /resultats  → résultats compare/verify
#
VOLUME ["/data", "/bases", "/pipelines", "/resultats"]

# RESULTATS_DIR par défaut redirigé vers /resultats (volume monté)
ENV RESULTATS_DIR=/resultats

ENTRYPOINT ["/entrypoint.sh"]
CMD ["help"]

--- Fichier : README.md ---
# hash-tool - Vérification d'intégrité de dossiers

voir ReadTheDocs 



--- Fichier : docker-compose.yml ---
# docker-compose.yml - hash_tool
#
# Cas d'usage typiques :
#   docker compose run --rm integrity compute /data /bases/hashes.b3
#   docker compose run --rm integrity verify  /bases/hashes.b3 /data
#   docker compose run --rm integrity compare /bases/old.b3 /bases/new.b3
#   docker compose run --rm pipeline
#
# Adapter les volumes (section x-volumes) selon l'environnement :
#   - Windows/WSL   : /mnt/c/Users/TonNom/...
#   - NAS Synology  : /volume1/...
#   - Serveur Debian: /srv/...

# == Chemins à adapter ========================================================
x-volumes:
  data:      &vol-data      /chemin/vers/donnees     # données à hacher (lecture seule)
  bases:     &vol-bases     /chemin/vers/bases        # fichiers .b3
  pipelines: &vol-pipelines /chemin/vers/pipelines   # fichiers pipeline.json
  resultats: &vol-resultats /chemin/vers/resultats   # résultats compare/verify

# == Services =================================================================
services:

  # Service principal - compute / verify / compare
  integrity:
    image: hash_tool
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - *vol-data:/data:ro
      - *vol-bases:/bases
      - *vol-resultats:/resultats
    environment:
      - RESULTATS_DIR=/resultats
    # Pas de commande par défaut - passer la commande à docker compose run
    # Ex : docker compose run --rm integrity verify /bases/hashes.b3 /data

  # Service pipeline - exécute runner.sh avec pipeline.json monté
  pipeline:
    image: hash_tool
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - *vol-data:/data:ro
      - *vol-bases:/bases
      - *vol-pipelines:/pipelines
      - *vol-resultats:/resultats
    environment:
      - RESULTATS_DIR=/resultats
    command: ["runner", "/pipelines/pipeline.json"]

  # Service cron - vérification périodique (optionnel)
  # Nécessite : docker compose up -d cron
  cron:
    image: hash_tool
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - *vol-data:/data:ro
      - *vol-bases:/bases:ro
      - *vol-resultats:/resultats
    environment:
      - RESULTATS_DIR=/resultats
      - CRON_SCHEDULE=0 3 * * *       # 03h00 chaque nuit
      - CRON_BASE=/bases/hashes.b3    # base à vérifier
    # Le service cron tourne en boucle - nécessite l'image étendue avec crond
    # Voir docs/docker-cron.md pour le setup complet
    command: ["shell"]
    profiles: ["cron"]   # non démarré par défaut (docker compose --profile cron up)
    restart: unless-stopped


--- Fichier : hash-tool ---
#!/usr/bin/env bash
# hash-tool - Interface CLI unique pour hash_tool
#
# Abstraction de la couche d'exécution : détecte automatiquement si l'exécution
# native est possible (b3sum disponible), sinon délègue à Docker.
#
# Usage :
#   hash-tool <commande> [options]
#
# Commandes :
#   compute     Calcule les empreintes d'un dossier.
#   verify      Vérifie l'intégrité d'un dossier à partir d'une base.
#   compare     Compare deux bases d'empreintes.
#   runner      Exécute un pipeline JSON.
#   list        Liste les bases d'empreintes disponibles dans un dossier.
#   diff        Affiche les différences entre une base et un dossier courant.
#   stats       Affiche des statistiques sur une base d'empreintes.
#   check-env   Analyse l'environnement d'exécution (natif ou conteneur).
#   version     Affiche la version du logiciel.
#   help        Affiche cette aide.
#
# Dépendances natives : bash >= 4, b3sum, jq
# Dépendances Docker  : docker (image hash_tool)

set -euo pipefail

# == Version ===================================================================

HASH_TOOL_VERSION="2.0.0"

# == Résolution du répertoire du script ========================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
INTEGRITY="$SCRIPT_DIR/src/integrity.sh"
RUNNER="$SCRIPT_DIR/runner.sh"
DOCKER_IMAGE="${HASH_TOOL_DOCKER_IMAGE:-hash_tool}"

# == Couleurs ==================================================================

_red()    { echo -e "\033[0;31m$*\033[0m"; }
_green()  { echo -e "\033[0;32m$*\033[0m"; }
_yellow() { echo -e "\033[0;33m$*\033[0m"; }
_cyan()   { echo -e "\033[0;36m$*\033[0m"; }
_dim()    { echo -e "\033[2m$*\033[0m"; }
_bold()   { echo -e "\033[1m$*\033[0m"; }

# == Détection de l'environnement ==============================================

_native_available() {
  command -v b3sum &>/dev/null \
    && command -v jq   &>/dev/null \
    && [ -f "$INTEGRITY" ] \
    && [ -x "$INTEGRITY" ]
}

_docker_available() {
  command -v docker &>/dev/null \
    && docker image inspect "$DOCKER_IMAGE" &>/dev/null 2>&1
}

_detect_exec_mode() {
  if _native_available; then
    echo "native"
  elif _docker_available; then
    echo "docker"
  else
    echo "none"
  fi
}

EXEC_MODE="$(_detect_exec_mode)"

# == Helpers ===================================================================

die() { echo "ERREUR : $*" >&2; exit 1; }

_require_native_or_die() {
  [ "$EXEC_MODE" = "native" ] || die "Exécution native requise pour cette commande (b3sum, jq, integrity.sh introuvables)."
}

# Argument parser : extrait les options nommées et les positionne dans des variables
# Usage : _parse_args "$@"  → positionne OPT_* variables
_parse_args() {
  OPT_DATA=""
  OPT_BASE=""
  OPT_OLD=""
  OPT_NEW=""
  OPT_PIPELINE=""
  OPT_SAVE=""
  OPT_META=""
  OPT_QUIET=0
  OPT_VERBOSE=0
  OPT_READONLY=0
  OPT_EXTRA=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      -data)      OPT_DATA="$2";     shift 2 ;;
      -base)      OPT_BASE="$2";     shift 2 ;;
      -old)       OPT_OLD="$2";      shift 2 ;;
      -new)       OPT_NEW="$2";      shift 2 ;;
      -pipeline)  OPT_PIPELINE="$2"; shift 2 ;;
      -save)      OPT_SAVE="$2";     shift 2 ;;
      -meta)      OPT_META="$2";     shift 2 ;;
      -quiet)     OPT_QUIET=1;       shift   ;;
      -verbose)   OPT_VERBOSE=1;     shift   ;;
      -readonly)  OPT_READONLY=1;    shift   ;;
      -help|--help|-h) return 99     ;;  # signal d'aide
      *)          OPT_EXTRA+=("$1"); shift   ;;
    esac
  done
}

# == Sidecar file ==============================================================

# _sidecar_write <b3_path> <data_dir> [meta_comment]
_sidecar_write() {
  local b3_path="$1"
  local data_dir="$2"
  local comment="${3:-}"
  local sidecar_path="${b3_path}.meta.json"

  local nb_files
  nb_files=$(wc -l < "$b3_path" 2>/dev/null || echo 0)

  jq -n \
    --arg version  "hash-tool v${HASH_TOOL_VERSION}" \
    --arg date     "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --arg comment  "$comment" \
    --arg dir      "$data_dir" \
    --argjson nb   "$nb_files" \
    --argjson ro   "$OPT_READONLY" \
    '{
      created_by: $version,
      date:       $date,
      comment:    $comment,
      parameters: {
        directory:  $dir,
        hash_algo:  "blake3",
        readonly:   ($ro == 1),
        nb_files:   $nb
      }
    }' > "$sidecar_path"

  echo "Sidecar : $sidecar_path"
}

# _sidecar_read <b3_path>  →  affiche le sidecar si présent
_sidecar_read() {
  local b3_path="$1"
  local sidecar_path="${b3_path}.meta.json"
  if [ -f "$sidecar_path" ]; then
    echo "--- Métadonnées (sidecar) ---"
    jq '.' "$sidecar_path" 2>/dev/null || cat "$sidecar_path"
    echo "-----------------------------"
  fi
}

# == Exécution native vs Docker ================================================

# Wrapper : exécute integrity.sh en natif ou via Docker
_run_integrity() {
  if [ "$EXEC_MODE" = "native" ]; then
    local quiet_flag=""
    (( OPT_QUIET )) && quiet_flag="--quiet"
    bash "$INTEGRITY" $quiet_flag "$@"
  elif [ "$EXEC_MODE" = "docker" ]; then
    _run_docker_integrity "$@"
  else
    die "Aucun environnement d'exécution disponible. Installez b3sum+jq ou construisez l'image Docker '${DOCKER_IMAGE}'."
  fi
}

# Wrapper Docker pour integrity.sh
_run_docker_integrity() {
  local args=("$@")
  # Volumes à monter selon la commande
  local volumes=()

  case "${args[0]}" in
    compute)
      volumes+=(-v "${args[1]}:/data:ro" -v "$(dirname "${args[2]}"):/bases")
      set -- compute /data "$(basename "${args[2]}")"
      ;;
    verify)
      local base_dir; base_dir="$(dirname "${args[1]}")"
      local base_name; base_name="$(basename "${args[1]}")"
      volumes+=(-v "${base_dir}:/bases:ro")
      [ -n "${args[2]:-}" ] && volumes+=(-v "${args[2]}:/data:ro")
      set -- verify "/bases/${base_name}" ${args[2]:+/data}
      ;;
    compare)
      local dir_a; dir_a="$(dirname "${args[1]}")"
      local dir_b; dir_b="$(dirname "${args[2]}")"
      volumes+=(-v "${dir_a}:/bases_a:ro" -v "${dir_b}:/bases_b:ro")
      if [ -n "${RESULTATS_DIR:-}" ]; then
        volumes+=(-v "${RESULTATS_DIR}:/resultats")
      fi
      set -- compare "/bases_a/$(basename "${args[1]}")" "/bases_b/$(basename "${args[2]}")"
      ;;
    *)
      die "Commande non supportée en mode Docker : ${args[0]}"
      ;;
  esac

  docker run --rm "${volumes[@]}" \
    -e RESULTATS_DIR="${RESULTATS_DIR:-/resultats}" \
    "$DOCKER_IMAGE" "$@"
}

# == Commandes =================================================================

# _check_help <subcmd> "$@"
# Si le premier argument positionnel est "help", affiche l'aide de la sous-commande
# et quitte. À appeler EN TÊTE de chaque cmd_* avant _parse_args.
_check_help() {
  local subcmd="$1"
  shift
  for arg in "$@"; do
    case "$arg" in
      help|--help|-h)
        cmd_help "$subcmd"
        exit 0
        ;;
    esac
  done
}

cmd_compute() {
  _check_help compute "$@"
  _parse_args "$@"

  # Vérification des paramètres obligatoires
  [ -n "$OPT_DATA" ] || die "compute : -data <dossier> requis."
  [ -d "$OPT_DATA" ] || die "compute : dossier introuvable : $OPT_DATA"

  # 1. Préparation du dossier de sauvegarde
  local save_dir="${OPT_SAVE:-.}"
  mkdir -p "$save_dir"
  
  # 2. Calcul du chemin ABSOLU du dossier de sauvegarde pour le sous-shell
  local save_dir_abs
  save_dir_abs="$(cd "$save_dir" && pwd)"
  
  # 3. Détermination du nom du fichier .b3
  local data_basename
  data_basename="$(basename "$(realpath "$OPT_DATA")")"
  local b3_file="${save_dir_abs}/hashes_${data_basename}.b3"

  # 4. Exécution dans un sous-shell
  # On utilise des guillemets autour de "$OPT_DATA" et "$b3_file" pour gérer les espaces
  (
    cd "$OPT_DATA" || exit 1
    _run_integrity compute "." "$b3_file"
  )

  # 5. Écriture du Sidecar (Métadonnées)
  # On vérifie si le fichier .b3 a bien été créé avant d'écrire le sidecar
  if [ -f "$b3_file" ]; then
    local data_path_abs
    data_path_abs="$(realpath "$OPT_DATA")"
    _sidecar_write "$b3_file" "$data_path_abs" "$OPT_META"
  fi
}

cmd_verify() {
  _check_help verify "$@"
  _parse_args "$@"
  [ -n "$OPT_BASE" ] || die "verify : -base <fichier.b3> requis."

  if [ -n "$OPT_SAVE" ]; then
    export RESULTATS_DIR="$OPT_SAVE"
    mkdir -p "$OPT_SAVE"
  fi

  _sidecar_read "$OPT_BASE"
  _run_integrity verify "$OPT_BASE" ${OPT_DATA:+"$OPT_DATA"}
}

cmd_compare() {
  _check_help compare "$@"
  _parse_args "$@"
  [ -n "$OPT_OLD" ] || die "compare : -old <ancienne.b3> requis."
  [ -n "$OPT_NEW" ] || die "compare : -new <nouvelle.b3> requis."

  if [ -n "$OPT_SAVE" ]; then
    export RESULTATS_DIR="$OPT_SAVE"
    mkdir -p "$OPT_SAVE"
  fi

  _sidecar_read "$OPT_OLD"
  _sidecar_read "$OPT_NEW"
  _run_integrity compare "$OPT_OLD" "$OPT_NEW"
}

cmd_runner() {
  _check_help runner "$@"
  _parse_args "$@"

  local pipeline_file="${OPT_PIPELINE:-${SCRIPT_DIR}/pipelines/pipeline.json}"
  [ -f "$pipeline_file" ] || die "runner : pipeline introuvable : $pipeline_file"

  if [ -n "$OPT_SAVE" ]; then
    export RESULTATS_DIR="$OPT_SAVE"
    mkdir -p "$OPT_SAVE"
  fi

  if [ "$EXEC_MODE" = "native" ]; then
    bash "$RUNNER" "$pipeline_file"
  elif [ "$EXEC_MODE" = "docker" ]; then
    local pipeline_dir; pipeline_dir="$(dirname "$(realpath "$pipeline_file")")"
    local pipeline_name; pipeline_name="$(basename "$pipeline_file")"
    docker run --rm \
      -v "${pipeline_dir}:/pipelines:ro" \
      -e RESULTATS_DIR="${RESULTATS_DIR:-/resultats}" \
      "$DOCKER_IMAGE" runner "/pipelines/${pipeline_name}"
  else
    die "Aucun environnement d'exécution disponible."
  fi
}

cmd_list() {
  _check_help list "$@"
  _parse_args "$@"
  local dir="${OPT_DATA:-${OPT_BASE:-.}}"

  echo "=== Bases d'empreintes dans : $dir ==="
  echo ""

  local found=0
  while IFS= read -r -d '' f; do
    found=1
    local nb_files
    nb_files=$(wc -l < "$f" 2>/dev/null || echo "?")
    local size
    size=$(du -sh "$f" 2>/dev/null | cut -f1 || echo "?")
    local sidecar_flag=""
    [ -f "${f}.meta.json" ] && sidecar_flag=" [+meta]"

    printf "  %-40s  %6s fichiers  %5s%s\n" "$(basename "$f")" "$nb_files" "$size" "$sidecar_flag"

    # Affiche le commentaire du sidecar si présent
    if [ -f "${f}.meta.json" ]; then
      local comment date
      comment=$(jq -r '.comment // empty' "${f}.meta.json" 2>/dev/null || true)
      date=$(jq -r '.date // empty' "${f}.meta.json" 2>/dev/null || true)
      [ -n "$comment" ] && printf "  %s→ %s (%s)\n" "   " "$comment" "$date"
    fi
  done < <(find "$dir" -maxdepth 2 -name "*.b3" -type f -print0 | sort -z)

  if (( found == 0 )); then
    echo "  Aucune base .b3 trouvée dans : $dir"
  fi
}

cmd_diff() {
  _parse_args "$@"
  [ -n "$OPT_BASE" ] || die "diff : -base <fichier.b3> requis."

  local workdir="${OPT_DATA:-.}"
  [ -d "$workdir" ] || die "diff : dossier introuvable : $workdir"

  echo "=== DIFF : $(basename "$OPT_BASE") vs $workdir ==="
  echo ""

  # Fichiers dans la base
  local tmp_base tmp_dir
  tmp_base=$(mktemp)
  tmp_dir=$(mktemp)
  trap 'rm -f "$tmp_base" "$tmp_dir"' EXIT

  # Extrait les chemins relatifs de la base (colonne 2+)
  awk '{ print substr($0,67) }' "$OPT_BASE" | sort > "$tmp_base"

  # Parcourt le dossier courant avec le même préfixe que la base
  local prefix
  prefix=$(awk '{ p=substr($0,67); sub(/[^/]+$/, "", p); print p; exit }' "$OPT_BASE" 2>/dev/null || echo "./")

  find "$workdir" -type f | sed "s|^${workdir%/}/||" | sed "s|^|${prefix}|" | sort > "$tmp_dir"

  local disparus nouveaux
  disparus=$(comm -23 "$tmp_base" "$tmp_dir" | wc -l)
  nouveaux=$(comm -13 "$tmp_base" "$tmp_dir" | wc -l)

  echo "  Fichiers disparus depuis la base : $disparus"
  if (( disparus > 0 )); then
    comm -23 "$tmp_base" "$tmp_dir" | while IFS= read -r f; do
      echo "    - $f"
    done
  fi
  echo ""
  echo "  Nouveaux fichiers non indexés : $nouveaux"
  if (( nouveaux > 0 )); then
    comm -13 "$tmp_base" "$tmp_dir" | while IFS= read -r f; do
      echo "    + $f"
    done
  fi

  rm -f "$tmp_base" "$tmp_dir"
  trap - EXIT
}

cmd_stats() {
  _parse_args "$@"
  local b3_file="${OPT_BASE:-${OPT_EXTRA[0]:-}}"
  [ -n "$b3_file" ] || die "stats : -base <fichier.b3> requis."
  [ -f "$b3_file" ] || die "stats : fichier introuvable : $b3_file"

  echo "=== Statistiques : $(basename "$b3_file") ==="
  echo ""

  local nb_files
  nb_files=$(wc -l < "$b3_file")
  local file_size
  file_size=$(du -sh "$b3_file" | cut -f1)
  local b3_path
  b3_path=$(realpath "$b3_file")

  printf "  Fichier base     : %s\n" "$b3_path"
  printf "  Taille fichier   : %s\n" "$file_size"
  printf "  Fichiers indexés : %s\n" "$nb_files"

  # Extensions les plus fréquentes
  echo ""
  echo "  Extensions :"
  awk '{print $NF}' "$b3_file" \
    | grep -oE '\.[^./]+$' \
    | sort | uniq -c | sort -rn | head -10 \
    | while read -r count ext; do
        printf "    %-12s  %5s fichiers\n" "$ext" "$count"
      done

  # Sidecar
  if [ -f "${b3_file}.meta.json" ]; then
    echo ""
    _sidecar_read "$b3_file"
  fi
}

cmd_check_env() {
  echo "=== check-env : Analyse de l'environnement ==="
  echo ""

  # b3sum
  if command -v b3sum &>/dev/null; then
    local b3ver; b3ver=$(b3sum --version 2>/dev/null || echo "version inconnue")
    _green "  [OK] b3sum disponible : $b3ver"
  else
    _red   "  [KO] b3sum introuvable (requis pour exécution native)"
  fi

  # jq
  if command -v jq &>/dev/null; then
    local jqver; jqver=$(jq --version 2>/dev/null || echo "version inconnue")
    _green "  [OK] jq disponible : $jqver"
  else
    _red   "  [KO] jq introuvable (requis pour pipelines JSON)"
  fi

  # bash
  _green "  [OK] bash $BASH_VERSION"
  if (( BASH_VERSINFO[0] < 4 )); then
    _red "  [KO] bash >= 4 requis"
  fi

  # integrity.sh
  if [ -f "$INTEGRITY" ] && [ -x "$INTEGRITY" ]; then
    _green "  [OK] integrity.sh présent et exécutable : $INTEGRITY"
  else
    _red   "  [KO] integrity.sh introuvable ou non exécutable : $INTEGRITY"
  fi

  # runner.sh
  if [ -f "$RUNNER" ] && [ -x "$RUNNER" ]; then
    _green "  [OK] runner.sh présent et exécutable : $RUNNER"
  else
    _red   "  [KO] runner.sh introuvable ou non exécutable : $RUNNER"
  fi

  # Docker
  echo ""
  if command -v docker &>/dev/null; then
    local dver; dver=$(docker --version 2>/dev/null || echo "version inconnue")
    _green "  [OK] Docker disponible : $dver"
    if docker image inspect "$DOCKER_IMAGE" &>/dev/null 2>&1; then
      _green "  [OK] Image Docker '$DOCKER_IMAGE' disponible"
    else
      _yellow "  [--] Image Docker '$DOCKER_IMAGE' absente (docker build requis)"
    fi
  else
    _yellow "  [--] Docker non disponible (optionnel)"
  fi

  echo ""
  echo "  Mode d'exécution sélectionné : $(_bold "$EXEC_MODE")"

  case "$EXEC_MODE" in
    native) _green "  → Exécution native active" ;;
    docker) _yellow "  → Exécution Docker active (fallback)" ;;
    none)   _red    "  → Aucun environnement disponible - impossible d'exécuter" ;;
  esac
}

cmd_version() {
  echo "hash-tool v${HASH_TOOL_VERSION}"
  echo "Moteur : BLAKE3 (b3sum)"
  if command -v b3sum &>/dev/null; then
    b3sum --version 2>/dev/null || true
  fi
}

cmd_help() {
  local subcmd="${1:-}"

  case "$subcmd" in
    compute)
      cat <<'EOF'
hash-tool compute -data <dossier> [-save <dossier>] [-meta <texte>] [-quiet] [-readonly]

  Calcule les empreintes BLAKE3 de tous les fichiers du dossier source.
  Génère un fichier .b3 et un sidecar .meta.json associé.

  Options :
    -data <dossier>   Dossier à analyser (requis).
    -save <dossier>   Dossier de sortie pour le .b3 (défaut : répertoire courant).
    -meta <texte>     Commentaire stocké dans le sidecar JSON.
    -quiet            Mode silencieux (pas de sortie terminal).
    -readonly         Documente le flag dans le sidecar (pas d'effet sur b3sum).

  Exemple :
    hash-tool compute -data ./donnees -save ./bases -meta "Snapshot initial"
EOF
      ;;
    verify)
      cat <<'EOF'
hash-tool verify -base <fichier.b3> [-data <dossier>] [-save <dossier>] [-quiet]

  Vérifie l'intégrité d'un dossier à partir d'une base d'empreintes.

  Options :
    -base <fichier.b3>  Base d'empreintes (requis).
    -data <dossier>     Dossier à vérifier (défaut : répertoire courant au moment du compute).
    -save <dossier>     Dossier de sortie des résultats (surcharge RESULTATS_DIR).
    -quiet              Mode silencieux.

  Exemple :
    hash-tool verify -base ./bases/hashes_donnees.b3 -data ./donnees
EOF
      ;;
    compare)
      cat <<'EOF'
hash-tool compare -old <ancienne.b3> -new <nouvelle.b3> [-save <dossier>]

  Compare deux bases d'empreintes et produit un rapport HTML.

  Options :
    -old <ancienne.b3>  Ancienne base (référence).
    -new <nouvelle.b3>  Nouvelle base (à comparer).
    -save <dossier>     Dossier de sortie des résultats.

  Exemple :
    hash-tool compare -old snap1.b3 -new snap2.b3 -save ./rapports
EOF
      ;;
    runner)
      cat <<'EOF'
hash-tool runner -pipeline <fichier.json> [-save <dossier>]

  Exécute un pipeline JSON définissant une suite d'opérations.
  Format pipeline : voir pipelines/pipeline-amelioree.json

  Options :
    -pipeline <fichier.json>  Fichier pipeline (défaut : pipelines/pipeline.json).
    -save <dossier>           Dossier de résultats global (surcharge RESULTATS_DIR).

  Exemple :
    hash-tool runner -pipeline ./pipelines/mon_pipeline.json -save ./resultats
EOF
      ;;
    list)
      cat <<'EOF'
hash-tool list [-data <dossier>]

  Liste toutes les bases .b3 disponibles dans un dossier, avec leurs
  métadonnées sidecar si présentes.

  Options :
    -data <dossier>  Dossier à parcourir (défaut : répertoire courant).

  Exemple :
    hash-tool list -data ./bases
EOF
      ;;
    diff)
      cat <<'EOF'
hash-tool diff -base <fichier.b3> [-data <dossier>]

  Affiche les différences entre une base d'empreintes et l'état actuel
  d'un dossier (fichiers disparus, nouveaux fichiers non indexés).
  Ne recalcule pas les hashes - uniquement comparaison des chemins.

  Options :
    -base <fichier.b3>  Base de référence (requis).
    -data <dossier>     Dossier courant à comparer (défaut : .).

  Exemple :
    hash-tool diff -base ./bases/hashes_donnees.b3 -data ./donnees
EOF
      ;;
    stats)
      cat <<'EOF'
hash-tool stats -base <fichier.b3>

  Affiche des statistiques sur une base d'empreintes :
  nombre de fichiers, taille, distribution des extensions, métadonnées sidecar.

  Options :
    -base <fichier.b3>  Base à analyser (requis).

  Exemple :
    hash-tool stats -base ./bases/hashes_donnees.b3
EOF
      ;;
    check-env)
      cat <<'EOF'
hash-tool check-env

  Analyse l'environnement d'exécution : vérifie la disponibilité de
  b3sum, jq, bash >= 4, integrity.sh, runner.sh et Docker.
  Indique le mode d'exécution sélectionné (natif ou Docker).
EOF
      ;;
    *)
      cat <<EOF
hash-tool v${HASH_TOOL_VERSION} - Vérification d'intégrité BLAKE3

Usage : hash-tool <commande> [options]

Commandes :
  compute     Calcule les empreintes d'un dossier.
  verify      Vérifie l'intégrité d'un dossier à partir d'une base.
  compare     Compare deux bases d'empreintes.
  runner      Exécute un pipeline JSON.
  list        Liste les bases d'empreintes disponibles.
  diff        Affiche les différences entre une base et un dossier.
  stats       Affiche des statistiques sur une base.
  check-env   Analyse l'environnement d'exécution.
  version     Affiche la version.
  help        Affiche cette aide (ou 'help <commande>' pour le détail).

Options générales :
  -data <chemin>      Dossier à analyser.
  -base <chemin>      Fichier base d'empreintes (.b3).
  -old <chemin>       Ancienne base (pour compare).
  -new <chemin>       Nouvelle base (pour compare).
  -pipeline <chemin>  Fichier pipeline JSON (pour runner).
  -save <chemin>      Dossier de sortie pour les résultats.
  -meta <texte>       Commentaire pour le sidecar JSON (compute).
  -quiet              Mode silencieux.
  -verbose            Mode verbeux.
  -readonly           Marque le compute comme lecture seule dans le sidecar.

Mode d'exécution : $EXEC_MODE
  L'interface reste identique quel que soit le mode d'exécution.
  Docker est utilisé en fallback si l'exécution native est impossible.

Exemples :
  hash-tool compute   -data ./donnees -save ./bases -meta "Snapshot initial"
  hash-tool verify    -base ./bases/hashes_donnees.b3 -data ./donnees
  hash-tool compare   -old ancien.b3 -new nouveau.b3 -save ./rapports
  hash-tool runner    -pipeline ./pipelines/pipeline.json
  hash-tool list      -data ./bases
  hash-tool diff      -base ./bases/hashes_donnees.b3 -data ./donnees
  hash-tool stats     -base ./bases/hashes_donnees.b3
  hash-tool check-env
EOF
      ;;
  esac
}

# == Dispatch ==================================================================

CMD="${1:-help}"
shift || true

case "$CMD" in
  compute)   cmd_compute   "$@" ;;
  verify)    cmd_verify    "$@" ;;
  compare)   cmd_compare   "$@" ;;
  runner)    cmd_runner    "$@" ;;
  list)      cmd_list      "$@" ;;
  diff)      cmd_diff      "$@" ;;
  stats)     cmd_stats     "$@" ;;
  check-env) cmd_check_env "$@" ;;
  version)   cmd_version   "$@" ;;
  help|-h|--help) cmd_help "$@" ;;
  *)
    echo "Commande inconnue : '$CMD'" >&2
    echo "Utiliser : hash-tool help" >&2
    exit 1
    ;;
esac


--- Fichier : mkdocs.yml ---
site_name: hash_tool
site_description: Vérification d'intégrité de fichiers par hachage BLAKE3
site_author: hash_tool
docs_dir: docs
site_dir: site

repo_url: https://github.com/hash_tool/hash_tool
repo_name: hash_tool/hash_tool
edit_uri: edit/main/docs/

theme:
  name: material
  language: fr
  palette:
    - scheme: slate
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-4
        name: Passer en mode clair
    - scheme: default
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-7
        name: Passer en mode sombre
  font:
    text: DM Sans
    code: JetBrains Mono
  features:
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.sections
    - navigation.expand
    - navigation.top
    - navigation.footer
    - search.suggest
    - search.highlight
    - content.code.copy
    - content.code.annotate
    - content.tabs.link
  icon:
    repo: fontawesome/brands/github

plugins:
  - search:
      lang: fr

markdown_extensions:
  - admonition
  - pymdownx.details
  - pymdownx.superfences
  - pymdownx.highlight:
      anchor_linenums: true
      line_spans: __span
      pygments_lang_class: true
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - pymdownx.tabbed:
      alternate_style: true
  - tables
  - attr_list
  - md_in_html
  - toc:
      permalink: true

extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/hash_tool/hash_tool

nav:
  - Accueil: index.md
  - Démarrage rapide: getting-started.md

  - Référence:
    - integrity.sh: reference/integrity-sh.md
    - runner.sh & pipeline.json: reference/runner-sh.md
    - Docker: reference/docker.md

  - Spécifications:
    - Format .b3: spec/b3-format.md
    - API interne: spec/api-interne.md

  - Guides:
    - VeraCrypt & disques multiples: guides/veracrypt.md
    - CI / Cron: guides/cron-ci.md
    - NAS Synology: guides/nas-synology.md

  - Développement:
    - Architecture: development/architecture.md
    - Contribuer & Tests: development/contributing.md
    - Roadmap & Positionnement: development/roadmap.md
    - Changelog: development/changelog.md

  - Aide:
    - Troubleshooting: troubleshooting /troubleshooting_1.md

--- Fichier : runner.sh ---
#!/usr/bin/env bash
# runner.sh - Exécuteur de pipeline integrity.sh depuis pipeline.json
#
# Supporte deux formats de pipeline :
#
#   Format legacy (rétrocompatible) :
#     { "pipeline": [ { "op": "compute", "source": "...", "bases": "...", "nom": "..." }, ... ] }
#
#   Format étendu (recommandé) :
#     { "pipeline": [ {
#         "type":        "compute",
#         "params":      { "input": "...", "output_dir": "...", "filename": "..." },
#         "options":     { "quiet": false, "verbose": false, "readonly": false },
#         "meta":        { "comment": "..." },
#         "description": "Texte explicatif de l'étape"
#       }, ... ] }
#
# Usage :
#   ./runner.sh                          # lit pipelines/pipeline.json
#   ./runner.sh /chemin/pipeline.json    # config explicite
#
# Dépendances : bash >= 4, jq, src/integrity.sh

set -euo pipefail

# == Chemins ===================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
INTEGRITY="$SCRIPT_DIR/src/integrity.sh"
CONFIG="${1:-$SCRIPT_DIR/pipelines/pipeline.json}"

# == Prérequis =================================================================

(( BASH_VERSINFO[0] >= 4 )) || { echo "ERREUR : bash >= 4 requis" >&2; exit 1; }

command -v jq &>/dev/null  || { echo "ERREUR : jq non trouvé (apt install jq)" >&2; exit 1; }
[ -f "$INTEGRITY" ]        || { echo "ERREUR : src/integrity.sh introuvable : $INTEGRITY" >&2; exit 1; }
[ -f "$CONFIG" ]           || { echo "ERREUR : config introuvable : $CONFIG" >&2; exit 1; }

# == Validation JSON ===========================================================

jq empty "$CONFIG" 2>/dev/null || { echo "ERREUR : JSON invalide : $CONFIG" >&2; exit 1; }

nb_ops=$(jq '.pipeline | length' "$CONFIG")
(( nb_ops > 0 )) || { echo "ERREUR : tableau .pipeline vide ou absent" >&2; exit 1; }

# == Fonctions utilitaires =====================================================

die() { echo "ERREUR : $*" >&2; exit 1; }

# Détecte si un bloc utilise le format legacy ("op") ou étendu ("type")
_bloc_format() {
  local idx="$1"
  local has_op has_type
  has_op=$(jq -r --argjson i "$idx" '.pipeline[$i].op // empty' "$CONFIG")
  has_type=$(jq -r --argjson i "$idx" '.pipeline[$i].type // empty' "$CONFIG")
  if [ -n "$has_type" ]; then
    echo "extended"
  elif [ -n "$has_op" ]; then
    echo "legacy"
  else
    echo "unknown"
  fi
}

# Lit un champ JSON obligatoire dans le format legacy
require_field() {
  local idx="$1" field="$2"
  local val
  val=$(jq -r --argjson i "$idx" '.pipeline[$i].'"$field" "$CONFIG")
  if [ "$val" = "null" ] || [ -z "$val" ]; then
    die "Bloc #$((idx+1)) : champ '$field' manquant ou vide."
  fi
  echo "$val"
}

# Lit un champ JSON optionnel - retourne "" si absent
optional_field() {
  local idx="$1" field="$2"
  local val
  val=$(jq -r --argjson i "$idx" '.pipeline[$i].'"$field // empty" "$CONFIG" 2>/dev/null || true)
  echo "${val:-}"
}

# Lit un champ dans params{} du format étendu (obligatoire)
require_param() {
  local idx="$1" param="$2"
  local val
  val=$(jq -r --argjson i "$idx" ".pipeline[\$i].params.${param} // empty" "$CONFIG" 2>/dev/null || true)
  [ -n "$val" ] || die "Bloc #$((idx+1)) : params.${param} manquant ou vide."
  echo "$val"
}

# Lit un champ dans params{} du format étendu (optionnel)
optional_param() {
  local idx="$1" param="$2"
  local val
  val=$(jq -r --argjson i "$idx" ".pipeline[\$i].params.${param} // empty" "$CONFIG" 2>/dev/null || true)
  echo "${val:-}"
}

# Lit un flag dans options{} du format étendu (retourne 0 si absent/false)
option_flag() {
  local idx="$1" opt="$2"
  local val
  val=$(jq -r --argjson i "$idx" ".pipeline[\$i].options.${opt} // false" "$CONFIG" 2>/dev/null || echo "false")
  [ "$val" = "true" ] && echo 1 || echo 0
}

# Lit le commentaire meta du format étendu
meta_comment() {
  local idx="$1"
  local val
  val=$(jq -r --argjson i "$idx" '.pipeline[$i].meta.comment // empty' "$CONFIG" 2>/dev/null || true)
  echo "${val:-}"
}

# == Opérations - Format legacy ================================================

run_compute_legacy() {
  local i="$1"
  local source bases nom
  source=$(require_field "$i" "source")
  bases=$(require_field "$i" "bases")
  nom=$(require_field "$i" "nom")

  echo "=== COMPUTE : $source ==="
  [ -d "$source" ] || die "Bloc #$((i+1)) compute : dossier source introuvable : $source"

  mkdir -p "$bases"
  local bases_abs
  bases_abs="$(cd "$bases" && pwd)"
  ( cd "$source" && "$INTEGRITY" compute . "$bases_abs/$nom" )
}

run_verify_legacy() {
  local i="$1"
  local source base
  source=$(require_field "$i" "source")
  base=$(require_field "$i" "base")

  echo "=== VERIFY : $source ==="
  [ -d "$source" ] || die "Bloc #$((i+1)) verify : dossier source introuvable : $source"
  [ -f "$base" ]   || die "Bloc #$((i+1)) verify : base .b3 introuvable : $base"

  local base_abs
  base_abs="$(cd "$(dirname "$base")" && pwd)/$(basename "$base")"
  ( cd "$source" && "$INTEGRITY" verify "$base_abs" ) || true
}

run_compare_legacy() {
  local i="$1"
  local base_a base_b
  base_a=$(require_field "$i" "base_a")
  base_b=$(require_field "$i" "base_b")

  echo "=== COMPARE : $(basename "$base_a") vs $(basename "$base_b") ==="
  [ -f "$base_a" ] || die "Bloc #$((i+1)) compare : base_a introuvable : $base_a"
  [ -f "$base_b" ] || die "Bloc #$((i+1)) compare : base_b introuvable : $base_b"

  local resultats_dir
  resultats_dir=$(optional_field "$i" "resultats")

  if [ -n "$resultats_dir" ]; then
    mkdir -p "$resultats_dir"
    local resultats_abs
    resultats_abs="$(cd "$resultats_dir" && pwd)"
    echo "    → résultats dans : $resultats_abs"
    RESULTATS_DIR="$resultats_abs" "$INTEGRITY" compare "$base_a" "$base_b"
  else
    "$INTEGRITY" compare "$base_a" "$base_b"
  fi
}

# == Opérations - Format étendu ================================================

run_compute_extended() {
  local i="$1"
  local input output_dir filename
  input=$(require_param "$i" "input")
  output_dir=$(require_param "$i" "output_dir")
  filename=$(require_param "$i" "filename")

  local quiet
  quiet=$(option_flag "$i" "quiet")
  local comment
  comment=$(meta_comment "$i")

  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== COMPUTE : $input ==="
  [ -n "$desc" ] && echo "    → $desc"
  [ -n "$comment" ] && echo "    → meta: $comment"

  [ -d "$input" ] || die "Bloc #$((i+1)) compute : dossier source introuvable : $input"

  mkdir -p "$output_dir"
  local output_abs
  output_abs="$(cd "$output_dir" && pwd)"

  local quiet_flag=""
  (( quiet )) && quiet_flag="--quiet"

  # shellcheck disable=SC2086
  ( cd "$input" && "$INTEGRITY" $quiet_flag compute . "$output_abs/$filename" )

  # Sidecar : génération si jq disponible et commentaire présent
  local b3_path="${output_abs}/${filename}"
  if [ -f "$b3_path" ] && command -v jq &>/dev/null; then
    local nb_files; nb_files=$(wc -l < "$b3_path" 2>/dev/null || echo 0)
    jq -n \
      --arg version  "hash-tool runner" \
      --arg date     "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
      --arg comment  "$comment" \
      --arg dir      "$input" \
      --argjson nb   "$nb_files" \
      '{
        created_by: $version,
        date:       $date,
        comment:    $comment,
        parameters: { directory: $dir, hash_algo: "blake3", nb_files: $nb }
      }' > "${b3_path}.meta.json"
    echo "    → sidecar : ${b3_path}.meta.json"
  fi
}

run_verify_extended() {
  local i="$1"
  local input base
  input=$(require_param "$i" "input")
  base=$(require_param "$i" "base")

  local quiet; quiet=$(option_flag "$i" "quiet")
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== VERIFY : $input ==="
  [ -n "$desc" ] && echo "    → $desc"

  [ -d "$input" ] || die "Bloc #$((i+1)) verify : dossier source introuvable : $input"
  [ -f "$base" ]  || die "Bloc #$((i+1)) verify : base .b3 introuvable : $base"

  local base_abs
  base_abs="$(cd "$(dirname "$base")" && pwd)/$(basename "$base")"

  local quiet_flag=""
  (( quiet )) && quiet_flag="--quiet"

  # shellcheck disable=SC2086
  ( cd "$source" && "$INTEGRITY" verify "$base_abs" ) || true
}

run_compare_extended() {
  local i="$1"
  local input ref_base output_dir
  input=$(require_param "$i" "input")
  ref_base=$(require_param "$i" "reference")
  output_dir=$(optional_param "$i" "output_dir")

  local quiet; quiet=$(option_flag "$i" "quiet")
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== COMPARE : $(basename "$ref_base") vs $(basename "$input") ==="
  [ -n "$desc" ] && echo "    → $desc"

  [ -f "$ref_base" ] || die "Bloc #$((i+1)) compare : référence introuvable : $ref_base"
  [ -f "$input" ]    || die "Bloc #$((i+1)) compare : base courante introuvable : $input"

  local quiet_flag=""
  (( quiet )) && quiet_flag="--quiet"

  if [ -n "$output_dir" ]; then
    mkdir -p "$output_dir"
    local output_abs
    output_abs="$(cd "$output_dir" && pwd)"
    echo "    → résultats dans : $output_abs"
    # shellcheck disable=SC2086
    RESULTATS_DIR="$output_abs" "$INTEGRITY" $quiet_flag compare "$ref_base" "$input"
  else
    # shellcheck disable=SC2086
    "$INTEGRITY" $quiet_flag compare "$ref_base" "$input"
  fi
}

# Pour les commandes sans effet réel dans le pipeline mais documentées
run_list_extended() {
  local i="$1"
  local input_dir
  input_dir=$(require_param "$i" "input_dir")
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== LIST : $input_dir ==="
  [ -n "$desc" ] && echo "    → $desc"

  find "$input_dir" -maxdepth 2 -name "*.b3" -type f | sort | while IFS= read -r f; do
    local nb; nb=$(wc -l < "$f" 2>/dev/null || echo "?")
    printf "  %-40s  %6s fichiers\n" "$(basename "$f")" "$nb"
  done
}

run_diff_extended() {
  local i="$1"
  local input ref_dir
  input=$(require_param "$i" "input")
  ref_dir=$(require_param "$i" "reference_dir")
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== DIFF : $(basename "$input") vs $ref_dir ==="
  [ -n "$desc" ] && echo "    → $desc"
  [ -f "$input" ]  || die "Bloc #$((i+1)) diff : base introuvable : $input"
  [ -d "$ref_dir" ] || die "Bloc #$((i+1)) diff : dossier introuvable : $ref_dir"

  local tmp_base tmp_dir
  tmp_base=$(mktemp)
  tmp_dir=$(mktemp)
  trap 'rm -f "$tmp_base" "$tmp_dir"' EXIT

  awk '{ print substr($0,67) }' "$input" | sort > "$tmp_base"
  local prefix
  prefix=$(awk '{ p=substr($0,67); sub(/[^/]+$/, "", p); print p; exit }' "$input" 2>/dev/null || echo "./")
  find "$ref_dir" -type f | sed "s|^${ref_dir%/}/||" | sed "s|^|${prefix}|" | sort > "$tmp_dir"

  local dis; dis=$(comm -23 "$tmp_base" "$tmp_dir" | wc -l)
  local nou; nou=$(comm -13 "$tmp_base" "$tmp_dir" | wc -l)
  echo "  Disparus : $dis  |  Nouveaux : $nou"

  rm -f "$tmp_base" "$tmp_dir"
  trap - EXIT
}

run_stats_extended() {
  local i="$1"
  local input
  input=$(require_param "$i" "input")
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)

  echo "=== STATS : $(basename "$input") ==="
  [ -n "$desc" ] && echo "    → $desc"
  [ -f "$input" ] || die "Bloc #$((i+1)) stats : base introuvable : $input"

  local nb; nb=$(wc -l < "$input")
  local sz; sz=$(du -sh "$input" | cut -f1)
  echo "  Fichiers indexés : $nb  |  Taille : $sz"
}

run_checkenv_extended() {
  local i="$1"
  local desc
  desc=$(jq -r --argjson i "$i" '.pipeline[$i].description // empty' "$CONFIG" 2>/dev/null || true)
  echo "=== CHECK-ENV ==="
  [ -n "$desc" ] && echo "    → $desc"
  command -v b3sum &>/dev/null && echo "  b3sum : OK" || echo "  b3sum : KO"
  command -v jq    &>/dev/null && echo "  jq    : OK" || echo "  jq    : KO"
}

run_version_extended() {
  local i="$1"
  echo "=== VERSION ==="
  command -v b3sum &>/dev/null && b3sum --version || echo "b3sum non disponible"
}

# == Dispatch par bloc =========================================================

dispatch_bloc() {
  local i="$1"
  local fmt
  fmt=$(_bloc_format "$i")

  case "$fmt" in
    legacy)
      local op
      op=$(jq -r --argjson i "$i" '.pipeline[$i].op' "$CONFIG")
      if [ "$op" = "null" ] || [ -z "$op" ]; then
        die "Bloc #$((i+1)) : champ 'op' manquant."
      fi
      case "$op" in
        compute) run_compute_legacy "$i" ;;
        verify)  run_verify_legacy  "$i" ;;
        compare) run_compare_legacy "$i" ;;
        *)       die "Bloc #$((i+1)) : opération inconnue : '$op'" ;;
      esac
      ;;
    extended)
      local type
      type=$(jq -r --argjson i "$i" '.pipeline[$i].type' "$CONFIG")
      case "$type" in
        compute)   run_compute_extended   "$i" ;;
        verify)    run_verify_extended    "$i" ;;
        compare)   run_compare_extended   "$i" ;;
        list)      run_list_extended      "$i" ;;
        diff)      run_diff_extended      "$i" ;;
        stats)     run_stats_extended     "$i" ;;
        check-env) run_checkenv_extended  "$i" ;;
        version)   run_version_extended   "$i" ;;
        runner)    die "Bloc #$((i+1)) : 'runner' imbriqué non supporté." ;;
        *)         die "Bloc #$((i+1)) : type inconnu : '$type'" ;;
      esac
      ;;
    *)
      die "Bloc #$((i+1)) : ni 'op' (legacy) ni 'type' (étendu) trouvé."
      ;;
  esac
}

# == Main ======================================================================

echo "=== PIPELINE DÉMARRÉ : $(date) ==="
echo "=== Config : $CONFIG ($nb_ops opération(s)) ==="
echo ""

for (( i=0; i<nb_ops; i++ )); do
  dispatch_bloc "$i"
  echo ""
done

echo "=== PIPELINE TERMINÉ : $(date) ==="

--- Fichier : mon_dossier/_data-destination/lorem-ipsum-01-modif.txt ---
Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of "de Finibus Bonorum et Malorum" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, "Lorem ipsum dolor sit amet..", comes from a line in section 1.10.32.

fichier modifié



--- Fichier : mon_dossier/_data-destination/lorem-ipsum-02.txt ---
There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.



--- Fichier : mon_dossier/_data-destination/lorem-ipsum-03.txt ---
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.



--- Fichier : mon_dossier/_data-destination/lorem-ipsum-04.txt ---
It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).



--- Fichier : mon_dossier/_data-source/lorem-ipsum-01-modif.txt ---
Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of "de Finibus Bonorum et Malorum" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, "Lorem ipsum dolor sit amet..", comes from a line in section 1.10.32.



--- Fichier : mon_dossier/_data-source/lorem-ipsum-02.txt ---
There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.



--- Fichier : mon_dossier/_data-source/lorem-ipsum-03.txt ---
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.



--- Fichier : mon_dossier/_data-source/lorem-ipsum-04.txt ---
It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).



--- Fichier : mon_dossier/bases/hashes_destination.b3.meta.json ---
{
  "created_by": "hash-tool v2.0.0",
  "date": "2026-02-26T14:26:56Z",
  "comment": "destination, base initiale",
  "parameters": {
    "directory": "/media/veracrypt1/partition_laptop/divers/config ordinateur/2 en cours -- 2025-00-00 -- backup/d11 installation/hash_tool/mon_dossier/destination",
    "hash_algo": "blake3",
    "readonly": false,
    "nb_files": 4
  }
}


--- Fichier : mon_dossier/bases/hashes_source.b3.meta.json ---
{
  "created_by": "hash-tool v2.0.0",
  "date": "2026-02-26T14:27:09Z",
  "comment": "source, base initiale",
  "parameters": {
    "directory": "/media/veracrypt1/partition_laptop/divers/config ordinateur/2 en cours -- 2025-00-00 -- backup/d11 installation/hash_tool/mon_dossier/source",
    "hash_algo": "blake3",
    "readonly": false,
    "nb_files": 4
  }
}


--- Fichier : tests/run_tests.sh ---
#!/usr/bin/env bash
# run_tests.sh - suite de tests automatisée pour integrity.sh
# Usage    : cd tests && ./run_tests.sh
# Prérequis: b3sum, stat, du ; integrity.sh dans ../src/

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
INTEGRITY="$SCRIPT_DIR/../src/integrity.sh"
WORKDIR="$(mktemp -d /tmp/integrity-test.XXXXXX)"
export RESULTATS_DIR="$WORKDIR/resultats"

GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

PASS=0; FAIL=0; TOTAL=0

pass() { echo -e "${GREEN}  PASS${NC} - $1"; PASS=$((PASS+1)); TOTAL=$((TOTAL+1)); }
fail() { echo -e "${RED}  FAIL${NC} - $1"; FAIL=$((FAIL+1)); TOTAL=$((TOTAL+1)); }

assert_exit_zero()    { local l="$1"; shift; if "$@" >/dev/null 2>&1;  then pass "$l"; else fail "$l"; fi; }
assert_exit_nonzero() { local l="$1"; shift; if ! "$@" >/dev/null 2>&1; then pass "$l"; else fail "$l"; fi; }

assert_contains() {
  local label="$1" pattern="$2" output="$3"
  if echo "$output" | grep -q "$pattern"; then pass "$label"; else fail "$label (pattern: '$pattern' absent)"; fi
}

assert_not_contains() {
  local label="$1" pattern="$2" output="$3"
  if ! echo "$output" | grep -q "$pattern"; then pass "$label"; else fail "$label (pattern: '$pattern' présent à tort)"; fi
}

assert_line_count() {
  local label="$1" expected="$2" file="$3"
  local actual; actual=$(wc -l < "$file")
  if [ "$actual" -eq "$expected" ]; then pass "$label"; else fail "$label (attendu: $expected, obtenu: $actual)"; fi
}

assert_file_exists() {
  local label="$1" file="$2"
  if [ -f "$file" ]; then pass "$label"; else fail "$label (absent : $file)"; fi
}

assert_file_absent() {
  local label="$1" file="$2"
  if [ ! -f "$file" ]; then pass "$label"; else fail "$label (présent à tort : $file)"; fi
}

setup() {
  mkdir -p "$WORKDIR/data/sub"
  echo "contenu alpha" > "$WORKDIR/data/alpha.txt"
  echo "contenu beta"  > "$WORKDIR/data/beta.txt"
  echo "contenu gamma" > "$WORKDIR/data/gamma.txt"
  echo "contenu delta" > "$WORKDIR/data/sub/delta.txt"
}

teardown() { rm -rf "$WORKDIR"; }

run_tests() {
  cd "$WORKDIR"

  echo ""
  echo "========================================"
  echo "  integrity.sh - suite de tests"
  echo "  Workdir : $WORKDIR"
  echo "========================================"
  echo ""

  echo "T00 - ShellCheck"
  if command -v shellcheck &>/dev/null; then
    assert_exit_zero "ShellCheck integrity.sh"       shellcheck "$INTEGRITY"
    assert_exit_zero "ShellCheck src/lib/core.sh"    shellcheck "$SCRIPT_DIR/../src/lib/core.sh"
    assert_exit_zero "ShellCheck src/lib/ui.sh"      shellcheck "$SCRIPT_DIR/../src/lib/ui.sh"
    assert_exit_zero "ShellCheck src/lib/report.sh"  shellcheck "$SCRIPT_DIR/../src/lib/report.sh"
    assert_exit_zero "ShellCheck src/lib/results.sh" shellcheck "$SCRIPT_DIR/../src/lib/results.sh"
    assert_exit_zero "ShellCheck runner.sh"          shellcheck "$SCRIPT_DIR/../runner.sh"
  else
    echo "  SKIP - shellcheck non installé"
  fi
  echo ""

  echo "T01 - Compute de base"
  bash "$INTEGRITY" compute ./data base_t01.b3 >/dev/null 2>&1
  assert_line_count "base_t01.b3 contient 4 lignes" 4 base_t01.b3
  assert_contains   "format <hash>  <chemin>"       "  ./data/" "$(head -1 base_t01.b3)"
  echo ""

  echo "T02 - Verify sans modification"
  local out_t02; out_t02=$(bash "$INTEGRITY" verify base_t01.b3 2>&1 || true)
  assert_not_contains "aucun FAILED" "FAILED" "$out_t02"
  assert_contains     "terminal OK"  "OK"     "$out_t02"
  local outdir_t02; outdir_t02=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t01*" 2>/dev/null | sort | tail -1)
  assert_file_exists  "recap.txt créé"              "${outdir_t02}/recap.txt"
  assert_file_absent  "failed.txt absent si 0 échec" "${outdir_t02}/failed.txt"
  echo ""

  echo "T03 - Verify après corruption"
  echo "contenu modifié" > data/beta.txt
  local out_t03; out_t03=$(bash "$INTEGRITY" verify base_t01.b3 2>&1 || true)
  assert_contains "ECHEC affiché"    "ECHEC"   "$out_t03"
  assert_contains "beta.txt FAILED"  "FAILED"  "$out_t03"
  local outdir_t03; outdir_t03=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t01*" 2>/dev/null | sort | tail -1)
  assert_file_exists "failed.txt créé"    "${outdir_t03}/failed.txt"
  assert_contains    "failed.txt beta"    "beta.txt" "$(cat "${outdir_t03}/failed.txt")"
  echo "contenu beta" > data/beta.txt
  echo ""

  echo "T04 - Verify après suppression"
  rm data/gamma.txt
  local out_t04; out_t04=$(bash "$INTEGRITY" verify base_t01.b3 2>&1 || true)
  assert_contains "gamma.txt FAILED" "FAILED" "$out_t04"
  echo "contenu gamma" > data/gamma.txt
  echo ""

  echo "T05 - Compare : aucune différence"
  bash "$INTEGRITY" compute ./data base_t05.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compare base_t01.b3 base_t05.b3 >/dev/null 2>&1
  local outdir_t05; outdir_t05=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t01*" 2>/dev/null | sort | tail -1)
  assert_file_exists "recap.txt"    "${outdir_t05}/recap.txt"
  assert_file_exists "modifies.b3"  "${outdir_t05}/modifies.b3"
  assert_file_exists "report.html"  "${outdir_t05}/report.html"
  assert_line_count  "modifies vide" 0 "${outdir_t05}/modifies.b3"
  assert_line_count  "disparus vide" 0 "${outdir_t05}/disparus.txt"
  assert_line_count  "nouveaux vide" 0 "${outdir_t05}/nouveaux.txt"
  echo ""

  echo "T06 - Compare : fichier modifié"
  echo "contenu beta modifié" > data/beta.txt
  bash "$INTEGRITY" compute ./data base_t06.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compare base_t01.b3 base_t06.b3 >/dev/null 2>&1
  local outdir_t06; outdir_t06=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t01*" 2>/dev/null | sort | tail -1)
  assert_contains "modifies contient beta" "beta.txt" "$(cat "${outdir_t06}/modifies.b3")"
  assert_file_exists "report.html généré" "${outdir_t06}/report.html"
  assert_contains    "report.html contient beta" "beta" "$(cat "${outdir_t06}/report.html")"
  echo "contenu beta" > data/beta.txt
  echo ""

  echo "T07 - Compare : suppression + ajout"
  bash "$INTEGRITY" compute ./data base_t07_old.b3 >/dev/null 2>&1
  rm data/alpha.txt
  echo "contenu epsilon" > data/epsilon.txt
  bash "$INTEGRITY" compute ./data base_t07_new.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compare base_t07_old.b3 base_t07_new.b3 >/dev/null 2>&1
  local outdir_t07; outdir_t07=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t07_old*" 2>/dev/null | sort | tail -1)
  assert_contains "disparus alpha"   "alpha.txt"   "$(cat "${outdir_t07}/disparus.txt")"
  assert_contains "nouveaux epsilon" "epsilon.txt" "$(cat "${outdir_t07}/nouveaux.txt")"
  echo "contenu alpha" > data/alpha.txt
  rm data/epsilon.txt
  echo ""

  echo "T08 - Robustesse : fichier avec espace"
  echo "contenu espace" > "data/fichier avec espace.txt"
  bash "$INTEGRITY" compute ./data base_t08.b3 >/dev/null 2>&1
  local out_t08; out_t08=$(bash "$INTEGRITY" verify base_t08.b3 2>&1 || true)
  assert_not_contains "aucun FAILED" "FAILED" "$out_t08"
  rm "data/fichier avec espace.txt"
  echo ""

  echo "T09 - Limite : dossier vide ignoré"
  mkdir data/dossier_vide
  bash "$INTEGRITY" compute ./data base_t09.b3 >/dev/null 2>&1
  assert_not_contains "dossier_vide absent" "dossier_vide" "$(cat base_t09.b3)"
  pass "comportement conforme"
  rmdir data/dossier_vide
  echo ""

  echo "T10 - Chemin absolu vs relatif"
  find "$WORKDIR/data" -type f -print0 | sort -z | xargs -0 b3sum > base_absolu.b3
  find ./data          -type f -print0 | sort -z | xargs -0 b3sum > base_relatif.b3
  assert_contains     "base absolue → chemin absolu"   "  /"      "$(head -1 base_absolu.b3)"
  assert_contains     "base relative → chemin relatif" "\./data/" "$(head -1 base_relatif.b3)"
  assert_not_contains "bases non interchangeables"     "$(head -1 base_absolu.b3)" "$(head -1 base_relatif.b3)"
  echo ""

  echo "T11 - ETA : base identique à référence"
  find ./data -type f -print0 | sort -z | xargs -0 b3sum > base_ref.b3
  bash "$INTEGRITY" compute ./data base_eta.b3 >/dev/null 2>&1
  assert_exit_zero    "base ETA == référence" diff base_ref.b3 base_eta.b3
  assert_not_contains "pas de ligne ETA"      "ETA" "$(cat base_eta.b3)"
  assert_not_contains "pas de \\r"            $'\r' "$(cat base_eta.b3)"
  echo ""

  echo "T12 - Mode --quiet"
  bash "$INTEGRITY" compute ./data base_t12.b3 >/dev/null 2>&1
  local out_quiet_ok; out_quiet_ok=$(bash "$INTEGRITY" --quiet verify base_t12.b3 2>&1 || true)
  assert_not_contains "--quiet OK : pas de stdout" "OK"        "$out_quiet_ok"
  assert_not_contains "--quiet OK : pas de stdout" "Résultats" "$out_quiet_ok"
  local outdir_t12; outdir_t12=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t12*" 2>/dev/null | sort | tail -1)
  assert_file_exists  "recap.txt produit --quiet" "${outdir_t12}/recap.txt"

  echo "contenu corrompu" > data/beta.txt
  local exit_quiet; bash "$INTEGRITY" --quiet verify base_t12.b3 >/dev/null 2>&1 && exit_quiet=0 || exit_quiet=$?
  if [ "$exit_quiet" -ne 0 ]; then pass "--quiet propage exit code"; else fail "--quiet propage exit code"; fi
  echo "contenu beta" > data/beta.txt

  local out_quiet_cmp; out_quiet_cmp=$(bash "$INTEGRITY" --quiet compute ./data base_t12c.b3 2>&1 || true)
  assert_not_contains "--quiet compute : pas de stdout" "Base enregistrée" "$out_quiet_cmp"
  echo ""

  echo "T13 - Horodatage anti-écrasement"
  bash "$INTEGRITY" compute ./data base_t13.b3 >/dev/null 2>&1
  bash "$INTEGRITY" verify base_t13.b3 >/dev/null 2>&1 || true
  sleep 1
  bash "$INTEGRITY" verify base_t13.b3 >/dev/null 2>&1 || true
  local nb_r; nb_r=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t13*" 2>/dev/null | wc -l)
  if [ "$nb_r" -ge 2 ]; then pass "deux dossiers distincts"; else fail "écrasement détecté ($nb_r dossier(s))"; fi
  echo ""

  echo "T14 - verify : dossier argument invalide"
  local out_t14; out_t14=$(bash "$INTEGRITY" verify base_t01.b3 /chemin/inexistant 2>&1 || true)
  assert_contains "ERREUR si dossier invalide" "ERREUR" "$out_t14"
  echo ""

  # =========================================================================
  # T15-T20 : cas limites supplémentaires
  # =========================================================================

  echo "T15 - Nom de fichier avec newline (mapfile -d '' doit tenir)"
  # Crée un fichier dont le nom contient un newline
  local newline_file
  newline_file="$(printf 'data/nom\nfichier.txt')"
  printf 'contenu newline' > "$newline_file"
  bash "$INTEGRITY" compute ./data base_t15.b3 >/dev/null 2>&1
  # Le .b3 doit contenir une ligne valide pour ce fichier
  local t15_lines; t15_lines=$(wc -l < base_t15.b3)
  if [ "$t15_lines" -ge 4 ]; then pass "T15 .b3 contient les 4+ fichiers dont le nom avec newline"; else fail "T15 .b3 incomplet ($t15_lines lignes)"; fi
  # Verify ne doit pas échouer
  local out_t15; out_t15=$(bash "$INTEGRITY" verify base_t15.b3 2>&1 || true)
  assert_not_contains "T15 verify sans FAILED" "FAILED" "$out_t15"
  rm -f "$newline_file"
  echo ""

  echo "T16 - Fichiers avec caractères HTML dans le nom"
  echo "contenu script" > "data/<script>.txt"
  echo "contenu amp"    > "data/a&b.txt"
  bash "$INTEGRITY" compute ./data base_t16.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compare base_t01.b3 base_t16.b3 >/dev/null 2>&1
  local outdir_t16; outdir_t16=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t01*" 2>/dev/null | sort | tail -1)
  assert_file_exists "T16 report.html généré" "${outdir_t16}/report.html"
  # Les chemins bruts doivent apparaître dans nouveaux.txt (sans échappement HTML)
  assert_contains "T16 <script>.txt dans nouveaux.txt" "<script>.txt" "$(cat "${outdir_t16}/nouveaux.txt")"
  assert_contains "T16 a&b.txt dans nouveaux.txt"     "a&b.txt"     "$(cat "${outdir_t16}/nouveaux.txt")"
  # report.html doit échapper correctement (pas de < ou & non échappés dans le rendu HTML)
  local html_t16; html_t16=$(cat "${outdir_t16}/report.html")
  # On vérifie que la séquence brute "<script>" n'est PAS telle quelle dans le HTML
  # (doit être &lt;script&gt; ou équivalent) - si html_escape n'existe pas, on signale
  if echo "$html_t16" | grep -q "&lt;script&gt;\|&amp;" 2>/dev/null; then
    pass "T16 échappement HTML présent dans report.html"
  elif echo "$html_t16" | grep -qF "<script>.txt"; then
    fail "T16 échappement HTML ABSENT dans report.html (XSS potentiel)"
  else
    pass "T16 report.html ne contient pas le nom brut (chemin absent ou échappé)"
  fi
  rm -f "data/<script>.txt" "data/a&b.txt"
  echo ""

  echo "T17 - Compare sans différence → report.html affiche IDENTIQUES"
  bash "$INTEGRITY" compute ./data base_t17a.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compute ./data base_t17b.b3 >/dev/null 2>&1
  bash "$INTEGRITY" compare base_t17a.b3 base_t17b.b3 >/dev/null 2>&1
  local outdir_t17; outdir_t17=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t17a*" 2>/dev/null | sort | tail -1)
  assert_file_exists "T17 report.html créé" "${outdir_t17}/report.html"
  local html_t17; html_t17=$(cat "${outdir_t17}/report.html")
  # Le rapport doit indiquer que les bases sont identiques (mot-clé "IDENTIQUES" ou "identique" ou "0 modification")
  if echo "$html_t17" | grep -qi "identique\|0 modification\|aucune différence\|aucune diff"; then
    pass "T17 report.html mentionne l'identité des bases"
  else
    fail "T17 report.html ne mentionne pas l'identité (cherchez le pattern attendu dans votre template)"
  fi
  echo ""

  echo "T18 - --quiet sur compare : stdout vide, fichiers produits"
  bash "$INTEGRITY" compute ./data base_t18a.b3 >/dev/null 2>&1
  echo "contenu beta modifié" > data/beta.txt
  bash "$INTEGRITY" compute ./data base_t18b.b3 >/dev/null 2>&1
  echo "contenu beta" > data/beta.txt
  local out_t18; out_t18=$(bash "$INTEGRITY" --quiet compare base_t18a.b3 base_t18b.b3 2>&1 || true)
  assert_not_contains "T18 --quiet compare : pas de stdout" "Résultats"  "$out_t18"
  assert_not_contains "T18 --quiet compare : pas de stdout" "Comparaison" "$out_t18"
  local outdir_t18; outdir_t18=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_base_t18a*" 2>/dev/null | sort | tail -1)
  assert_file_exists "T18 recap.txt produit en --quiet"    "${outdir_t18}/recap.txt"
  assert_file_exists "T18 modifies.b3 produit en --quiet"  "${outdir_t18}/modifies.b3"
  assert_file_exists "T18 report.html produit en --quiet"  "${outdir_t18}/report.html"
  assert_contains    "T18 modifies.b3 contient beta.txt"   "beta.txt" "$(cat "${outdir_t18}/modifies.b3")"
  echo ""

  echo "T19 - Lien symbolique dans le dossier source"
  # Crée un lien symbolique pointant vers un fichier existant
  ln -s "$WORKDIR/data/alpha.txt" "data/lien_alpha.txt" 2>/dev/null || true
  # Vérifie que compute ne plante pas
  local t19_exit=0
  bash "$INTEGRITY" compute ./data base_t19.b3 >/dev/null 2>&1 || t19_exit=$?
  if [ "$t19_exit" -eq 0 ]; then
    pass "T19 compute avec lien symbolique → exit 0"
    # Documente si le lien est suivi ou ignoré
    if grep -q "lien_alpha" base_t19.b3 2>/dev/null; then
      pass "T19 lien symbolique : suivi (inclus dans .b3)"
    else
      pass "T19 lien symbolique : ignoré (absent du .b3) - comportement à documenter"
    fi
  else
    fail "T19 compute avec lien symbolique → exit $t19_exit (plantage)"
  fi
  rm -f "data/lien_alpha.txt"
  echo ""

  echo "T20 - verify avec dossier source inexistant → exit 1"
  local out_t20; out_t20=$(bash "$INTEGRITY" verify base_t01.b3 /dossier/source/totalement/inexistant 2>&1 || true)
  assert_contains "T20 ERREUR si dossier source inexistant" "ERREUR" "$out_t20"
  # Doit être différent de T14 qui testait un dossier passé comme argument optionnel
  # Ici on s'assure que l'exit code est non-zéro
  local t20_exit=0
  bash "$INTEGRITY" verify base_t01.b3 /dossier/source/totalement/inexistant >/dev/null 2>&1 || t20_exit=$?
  if [ "$t20_exit" -ne 0 ]; then pass "T20 exit code non-zéro"; else fail "T20 exit code nul (attendu non-zéro)"; fi
  echo ""
}

# == Main ======================================================================

command -v b3sum &>/dev/null || { echo -e "${RED}ERREUR${NC} : b3sum non trouvé."; exit 1; }
[ -f "$INTEGRITY" ]          || { echo -e "${RED}ERREUR${NC} : integrity.sh introuvable : $INTEGRITY"; exit 1; }

setup
run_tests
teardown

echo "========================================"
if [ "$FAIL" -eq 0 ]; then
  echo -e "  ${GREEN}$PASS/$TOTAL tests passés${NC}"
else
  echo -e "  ${GREEN}$PASS${NC}/${TOTAL} passés - ${RED}$FAIL échec(s)${NC}"
fi
echo "========================================"
echo ""

[ "$FAIL" -eq 0 ]

--- Fichier : tests/run_tests_core.sh ---
#!/usr/bin/env bash
# run_tests_core.sh - Tests unitaires de src/lib/core.sh
# Usage    : cd tests && ./run_tests_core.sh
# Prérequis: bash >= 4, b3sum
#
# Source directement core.sh sans passer par integrity.sh.
# Chaque groupe de tests est isolé dans un sous-répertoire de WORKDIR.

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
WORKDIR="$(mktemp -d /tmp/integrity-core-test.XXXXXX)"
export RESULTATS_DIR="$WORKDIR/resultats"
mkdir -p "$RESULTATS_DIR"

# == Sourcing des modules =========================================================

# ui.sh doit être sourcé avant core.sh (die() est définie dans ui.sh)
# On redéfinit die() localement pour capturer les appels sans quitter le processus de test.
QUIET=0
die() { echo "die: $*" >&2; return 1; }

# shellcheck source=../src/lib/core.sh
source "$SCRIPT_DIR/../src/lib/core.sh"

# == Infrastructure de test =======================================================

GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

PASS=0; FAIL=0; TOTAL=0

pass() { echo -e "${GREEN}  PASS${NC} - $1"; PASS=$((PASS+1)); TOTAL=$((TOTAL+1)); }
fail() { echo -e "${RED}  FAIL${NC} - $1"; FAIL=$((FAIL+1)); TOTAL=$((TOTAL+1)); }

assert_exit_zero() {
  local label="$1"; shift
  local _rc=0
  ( set -e; "$@" ) >/dev/null 2>&1 || _rc=$?
  if [ "$_rc" -eq 0 ]; then pass "$label"; else fail "$label (exit $_rc attendu 0)"; fi
}

assert_exit_nonzero() {
  local label="$1"; shift
  local _rc=0
  ( set -e; "$@" ) >/dev/null 2>&1 || _rc=$?
  if [ "$_rc" -ne 0 ]; then pass "$label"; else fail "$label (exit 0 attendu non-zéro)"; fi
}

assert_contains() {
  local label="$1" pattern="$2" output="$3"
  if echo "$output" | grep -qF "$pattern"; then pass "$label"; else fail "$label (pattern absent: '$pattern')"; fi
}

assert_not_contains() {
  local label="$1" pattern="$2" output="$3"
  if ! echo "$output" | grep -qF "$pattern"; then pass "$label"; else fail "$label (pattern présent à tort: '$pattern')"; fi
}

assert_equals() {
  local label="$1" expected="$2" actual="$3"
  if [ "$actual" = "$expected" ]; then pass "$label"; else fail "$label (attendu: '$expected', obtenu: '$actual')"; fi
}

assert_numeric_eq() {
  local label="$1" expected="$2" actual="$3"
  if [ "$actual" -eq "$expected" ] 2>/dev/null; then pass "$label"; else fail "$label (attendu: $expected, obtenu: $actual)"; fi
}

# Hash b3sum 64 chars valide à partir d'une chaîne
_make_hash() { printf '%s' "$1" | b3sum --no-names; }

# Génère un fichier .b3 valide à partir d'un dossier
_compute_b3() {
  local dir="$1" out="$2"
  find "$dir" -type f -print0 | sort -z | xargs -0 b3sum > "$out"
}

teardown() { rm -rf "$WORKDIR"; }
trap teardown EXIT

# _run_core <fonction> [args...]
#
# Exécute une fonction de core.sh dans un sous-shell isolé.
# Nécessaire car die() fait return 1 et set -e tuerait le processus principal.
# Retourne le code de sortie de la fonction.
_run_core() {
  (
    QUIET=0
    die() { echo "die: $*" >&2; exit 1; }
    # shellcheck source=../src/lib/core.sh
    source "$SCRIPT_DIR/../src/lib/core.sh"
    "$@"
  ) 2>/dev/null
}

_run_core_stderr() {
  (
    QUIET=0
    die() { echo "die: $*" >&2; exit 1; }
    # shellcheck source=../src/lib/core.sh
    source "$SCRIPT_DIR/../src/lib/core.sh"
    "$@"
  ) 2>&1
}

# == T_CORE01 - core_assert_b3_valid =============================================

echo ""
echo "========================================"
echo "  T_CORE01 - core_assert_b3_valid"
echo "========================================"

# CU01 : fichier absent → exit 1
assert_exit_nonzero "CU01 fichier absent → exit 1" \
  _run_core core_assert_b3_valid "/tmp/inexistant_$$.b3"

# CU02 : répertoire passé → exit 1
assert_exit_nonzero "CU02 répertoire au lieu de fichier → exit 1" \
  _run_core core_assert_b3_valid "$WORKDIR"

# CU03 : fichier vide → exit 1
_cu03="$WORKDIR/vide.b3"; touch "$_cu03"
assert_exit_nonzero "CU03 fichier vide → exit 1" \
  _run_core core_assert_b3_valid "$_cu03"

# CU04 : format invalide (ligne sans hash) → exit 1
_cu04="$WORKDIR/bad_format.b3"; echo "ligne_sans_format_b3sum" > "$_cu04"
assert_exit_nonzero "CU04 format invalide → exit 1" \
  _run_core core_assert_b3_valid "$_cu04"

# CU05 : une seule ligne valide → exit 0
_cu05="$WORKDIR/valid_single.b3"
printf '%064d  ./fichier.txt\n' 0 > "$_cu05"
assert_exit_zero "CU05 une ligne valide → exit 0" \
  _run_core core_assert_b3_valid "$_cu05"

# CU06 : plusieurs lignes toutes valides → exit 0
_cu06="$WORKDIR/valid_multi.b3"
{ printf '%064d  ./alpha.txt\n' 0; printf '%064d  ./beta.txt\n' 1; } > "$_cu06"
assert_exit_zero "CU06 plusieurs lignes valides → exit 0" \
  _run_core core_assert_b3_valid "$_cu06"

# CU07 : lignes mixtes valides/invalides → exit 1
_cu07="$WORKDIR/mixed.b3"
{ printf '%064d  ./alpha.txt\n' 0; echo "ligne_invalide"; } > "$_cu07"
assert_exit_nonzero "CU07 lignes mixtes → exit 1" \
  _run_core core_assert_b3_valid "$_cu07"

# CU08 : label personnalisé transmis au message d'erreur
_cu08_err=$(_run_core_stderr core_assert_b3_valid "/inexistant.b3" "MON_LABEL" || true)
assert_contains "CU08 label personnalisé dans message d'erreur" "MON_LABEL" "$_cu08_err"

# CU09 : hash avec lettres minuscules (format b3sum réel)
_cu09="$WORKDIR/real_hash.b3"
printf 'abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890  ./x.txt\n' > "$_cu09"
assert_exit_zero "CU09 hash lettres minuscules réel → exit 0" \
  _run_core core_assert_b3_valid "$_cu09"

# CU10 : chemin avec espace dans la ligne valide → exit 0
_cu10="$WORKDIR/space_path.b3"
printf '%064d  ./fichier avec espace.txt\n' 0 > "$_cu10"
assert_exit_zero "CU10 chemin avec espace → exit 0" \
  _run_core core_assert_b3_valid "$_cu10"

# CU11 : chemin avec caractères spéciaux HTML → exit 0
_cu11="$WORKDIR/html_chars.b3"
printf '%064d  ./<script>.txt\n' 0 > "$_cu11"
assert_exit_zero "CU11 chemin avec <> → exit 0" \
  _run_core core_assert_b3_valid "$_cu11"

# == T_CORE02 - core_assert_target_valid =========================================

echo ""
echo "========================================"
echo "  T_CORE02 - core_assert_target_valid"
echo "========================================"

# CU12 : chemin inexistant → exit 1
assert_exit_nonzero "CU12 chemin inexistant → exit 1" \
  _run_core core_assert_target_valid "/chemin/totalement/inexistant_$$"

# CU13 : fichier régulier passé (pas un dossier) → exit 1
_cu13_f="$WORKDIR/un_fichier.txt"; touch "$_cu13_f"
assert_exit_nonzero "CU13 fichier régulier (pas dossier) → exit 1" \
  _run_core core_assert_target_valid "$_cu13_f"

# CU14 : dossier vide (aucun fichier) → exit 1
_cu14_d="$WORKDIR/dossier_vide"; mkdir -p "$_cu14_d"
assert_exit_nonzero "CU14 dossier vide → exit 1" \
  _run_core core_assert_target_valid "$_cu14_d"

# CU15 : dossier avec un fichier → exit 0
_cu15_d="$WORKDIR/dossier_un_fichier"; mkdir -p "$_cu15_d"
echo "contenu" > "$_cu15_d/f.txt"
assert_exit_zero "CU15 dossier avec fichier → exit 0" \
  _run_core core_assert_target_valid "$_cu15_d"

# CU16 : dossier avec sous-dossiers uniquement (pas de fichiers réguliers) → exit 1
_cu16_d="$WORKDIR/dossier_sous_dir"; mkdir -p "$_cu16_d/sub"
assert_exit_nonzero "CU16 dossier sans fichiers réguliers → exit 1" \
  _run_core core_assert_target_valid "$_cu16_d"

# CU17 : dossier avec fichiers dans sous-dossiers → exit 0
_cu17_d="$WORKDIR/dossier_sub_files"; mkdir -p "$_cu17_d/sub"
echo "contenu" > "$_cu17_d/sub/f.txt"
assert_exit_zero "CU17 dossier avec fichiers dans sous-dossiers → exit 0" \
  _run_core core_assert_target_valid "$_cu17_d"

# == T_CORE03 - core_compute =====================================================

echo ""
echo "========================================"
echo "  T_CORE03 - core_compute"
echo "========================================"

# Données de base pour les tests compute
_cu_compute_dir="$WORKDIR/data_compute"
mkdir -p "$_cu_compute_dir"
echo "alpha"   > "$_cu_compute_dir/alpha.txt"
echo "beta"    > "$_cu_compute_dir/beta.txt"
echo "gamma"   > "$_cu_compute_dir/gamma.txt"

# CU18 : fichier .b3 créé
_cu18_b3="$WORKDIR/cu18.b3"
( cd "$WORKDIR" && core_compute "data_compute" "$_cu18_b3" "" )
[ -f "$_cu18_b3" ] && pass "CU18 fichier .b3 créé" || fail "CU18 fichier .b3 absent"

# CU19 : N lignes pour N fichiers
_cu19_lines=$(wc -l < "$_cu18_b3")
assert_numeric_eq "CU19 3 lignes pour 3 fichiers" 3 "$_cu19_lines"

# CU20 : format <hash64>  <chemin>
_cu20_line=$(head -1 "$_cu18_b3")
if echo "$_cu20_line" | grep -qE '^[0-9a-f]{64}  .+'; then
  pass "CU20 format <hash64>  <chemin> correct"
else
  fail "CU20 format inattendu : $_cu20_line"
fi

# CU21 : chemin relatif préservé
assert_contains "CU21 chemin relatif dans .b3" "data_compute/" "$(cat "$_cu18_b3")"

# CU22 : fichier avec espace dans le nom
_cu22_d="$WORKDIR/data_cu22"; mkdir -p "$_cu22_d"
echo "espace" > "$_cu22_d/fichier avec espace.txt"
_cu22_b3="$WORKDIR/cu22.b3"
( cd "$WORKDIR" && core_compute "data_cu22" "$_cu22_b3" "" )
assert_contains "CU22 chemin avec espace dans .b3" "fichier avec espace.txt" "$(cat "$_cu22_b3")"
_cu22_lines=$(wc -l < "$_cu22_b3")
assert_numeric_eq "CU22 une seule ligne" 1 "$_cu22_lines"

# CU23 : fichier de taille zéro - doit figurer dans le .b3
_cu23_d="$WORKDIR/data_cu23"; mkdir -p "$_cu23_d"
touch "$_cu23_d/zero.bin"
_cu23_b3="$WORKDIR/cu23.b3"
( cd "$WORKDIR" && core_compute "data_cu23" "$_cu23_b3" "" )
assert_contains "CU23 fichier taille zéro présent dans .b3" "zero.bin" "$(cat "$_cu23_b3")"

# CU24 : callback appelé N fois
_cu24_d="$WORKDIR/data_cu24"; mkdir -p "$_cu24_d"
for i in 1 2 3 4 5; do echo "contenu $i" > "$_cu24_d/f${i}.txt"; done
_cu24_b3="$WORKDIR/cu24.b3"
_cu24_count=0
_counter_callback() { _cu24_count=$((_cu24_count + 1)); }
( cd "$WORKDIR" && core_compute "data_cu24" "$_cu24_b3" "_counter_callback" )
# Note : le callback étant appelé dans un sous-shell, on le ré-exécute en scope courant
_cu24_count=0
_cu24_b3b="$WORKDIR/cu24b.b3"
core_compute "$_cu24_d" "$_cu24_b3b" "_counter_callback"
assert_numeric_eq "CU24 callback appelé 5 fois" 5 "$_cu24_count"

# CU25 : callback reçoit les bons arguments (i, total, bytes_done, total_bytes, eta)
_cu25_d="$WORKDIR/data_cu25"; mkdir -p "$_cu25_d"
echo "data" > "$_cu25_d/f1.txt"
_cu25_b3="$WORKDIR/cu25.b3"
_cu25_args=()
_args_callback() { _cu25_args=("$@"); }
core_compute "$_cu25_d" "$_cu25_b3" "_args_callback"
if [ "${#_cu25_args[@]}" -eq 5 ]; then
  pass "CU25 callback reçoit 5 arguments"
else
  fail "CU25 callback reçoit ${#_cu25_args[@]} arguments (attendu 5)"
fi
# i=1, total=1
assert_numeric_eq "CU25 i=1"     1 "${_cu25_args[0]}"
assert_numeric_eq "CU25 total=1" 1 "${_cu25_args[1]}"

# CU26 : le fichier .b3 ne contient pas de lignes ETA ou \r
assert_not_contains "CU26 pas de 'ETA' dans .b3"  "ETA"  "$(cat "$_cu18_b3")"
if ! grep -qP '\r' "$_cu18_b3" 2>/dev/null; then
  pass "CU26 pas de \\r dans .b3"
else
  fail "CU26 \\r détecté dans .b3"
fi

# CU27 : idempotence - deux compute produisent des fichiers identiques
_cu27_b3a="$WORKDIR/cu27a.b3"; _cu27_b3b="$WORKDIR/cu27b.b3"
( cd "$WORKDIR" && core_compute "data_compute" "$_cu27_b3a" "" )
( cd "$WORKDIR" && core_compute "data_compute" "$_cu27_b3b" "" )
assert_exit_zero "CU27 idempotence - fichiers identiques" diff "$_cu27_b3a" "$_cu27_b3b"

# == T_CORE04 - core_verify ======================================================

echo ""
echo "========================================"
echo "  T_CORE04 - core_verify"
echo "========================================"

# Setup commun pour les tests verify
_cv_dir="$WORKDIR/data_verify"; mkdir -p "$_cv_dir"
echo "alpha"   > "$_cv_dir/alpha.txt"
echo "beta"    > "$_cv_dir/beta.txt"
echo "gamma"   > "$_cv_dir/gamma.txt"
echo "delta"   > "$_cv_dir/delta.txt"
_cv_b3="$WORKDIR/verify_base.b3"
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# CU28 : tous les fichiers intègres → exit 0, STATUS=OK
( cd "$_cv_dir" && core_verify "$_cv_b3" )
_cu28_exit=$?
assert_numeric_eq "CU28 exit 0 si tout OK" 0 "$_cu28_exit"
assert_equals "CU28 STATUS=OK" "OK" "$CORE_VERIFY_STATUS"

# CU29 : un fichier corrompu → exit 1, STATUS=ECHEC
echo "corrompu" > "$_cv_dir/beta.txt"
_cu29_exit=0
( cd "$_cv_dir" && core_verify "$_cv_b3" ) || _cu29_exit=$?
if [ "$_cu29_exit" -ne 0 ]; then pass "CU29 exit non-zéro si corruption"; else fail "CU29 doit détecter la corruption"; fi
assert_equals "CU29 STATUS=ECHEC" "ECHEC" "$CORE_VERIFY_STATUS"
assert_numeric_eq "CU29 NB_FAIL=1" 1 "$CORE_VERIFY_NB_FAIL"
echo "beta" > "$_cv_dir/beta.txt"
# Recalcul propre
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# CU30 : plusieurs fichiers corrompus
echo "corrompu_alpha" > "$_cv_dir/alpha.txt"
echo "corrompu_beta"  > "$_cv_dir/beta.txt"
_cu30_exit=0
( cd "$_cv_dir" && core_verify "$_cv_b3" ) || _cu30_exit=$?
if [ "$_cu30_exit" -ne 0 ]; then pass "CU30 exit non-zéro si 2 corruptions"; else fail "CU30 doit détecter 2 corruptions"; fi
if [ "$CORE_VERIFY_NB_FAIL" -ge 2 ]; then pass "CU30 NB_FAIL>=2"; else fail "CU30 NB_FAIL=$CORE_VERIFY_NB_FAIL (attendu >=2)"; fi
echo "alpha" > "$_cv_dir/alpha.txt"
echo "beta"  > "$_cv_dir/beta.txt"
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# CU31 : fichier supprimé → exit 1, chemin dans LINES_FAIL
rm "$_cv_dir/gamma.txt"
_cu31_exit=0
( cd "$_cv_dir" && core_verify "$_cv_b3" ) || _cu31_exit=$?
if [ "$_cu31_exit" -ne 0 ]; then pass "CU31 exit non-zéro si fichier supprimé"; else fail "CU31 doit détecter la suppression"; fi
assert_contains "CU31 gamma.txt dans LINES_FAIL" "gamma.txt" "$CORE_VERIFY_LINES_FAIL"
echo "gamma" > "$_cv_dir/gamma.txt"
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# CU32 : variables CORE_VERIFY_* non nulles en cas nominal
( cd "$_cv_dir" && core_verify "$_cv_b3" )
[ -n "$CORE_VERIFY_STATUS" ]     && pass "CU32 CORE_VERIFY_STATUS non nul"     || fail "CU32 CORE_VERIFY_STATUS vide"
[ -n "$CORE_VERIFY_NB_OK" ]      && pass "CU32 CORE_VERIFY_NB_OK non nul"      || fail "CU32 CORE_VERIFY_NB_OK vide"
[ -n "$CORE_VERIFY_NB_FAIL" ]    && pass "CU32 CORE_VERIFY_NB_FAIL non nul"    || fail "CU32 CORE_VERIFY_NB_FAIL vide"

# CU33 : NB_OK correct (4 fichiers)
assert_numeric_eq "CU33 NB_OK=4" 4 "$CORE_VERIFY_NB_OK"

# CU34 : LINES_FAIL contient le bon chemin après corruption d'un fichier spécifique
echo "corrompu" > "$_cv_dir/beta.txt"
( cd "$_cv_dir" && core_verify "$_cv_b3" ) || true
assert_contains "CU34 LINES_FAIL contient beta.txt" "beta.txt" "$CORE_VERIFY_LINES_FAIL"
echo "beta" > "$_cv_dir/beta.txt"
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# CU35 : STATUS=ERREUR si fichier illisible
chmod 000 "$_cv_dir/alpha.txt"
( cd "$_cv_dir" && core_verify "$_cv_b3" ) || true
if [ "$CORE_VERIFY_STATUS" = "ERREUR" ] || [ "$CORE_VERIFY_NB_FAIL" -gt 0 ]; then
  pass "CU35 fichier illisible détecté (ERREUR ou FAIL)"
else
  fail "CU35 fichier illisible non détecté (STATUS=$CORE_VERIFY_STATUS)"
fi
chmod 644 "$_cv_dir/alpha.txt"
( cd "$_cv_dir" && b3sum ./*.txt | sort > "$_cv_b3" )

# == T_CORE05 - core_compare =====================================================

echo ""
echo "========================================"
echo "  T_CORE05 - core_compare"
echo "========================================"

# Helper : crée deux bases à partir de deux dossiers et compare
_run_compare() {
  local old_dir="$1" new_dir="$2" old_b3="$3" new_b3="$4" outdir="$5"
  ( cd "$old_dir" && b3sum ./* 2>/dev/null | sort > "$old_b3" ) 2>/dev/null || true
  ( cd "$new_dir" && b3sum ./* 2>/dev/null | sort > "$new_b3" ) 2>/dev/null || true
  mkdir -p "$outdir"
  core_compare "$old_b3" "$new_b3" "$outdir"
}

# CU36 : bases identiques → tout à 0
_cu36_d="$WORKDIR/cu36"; mkdir -p "$_cu36_d"
echo "a" > "$_cu36_d/a.txt"; echo "b" > "$_cu36_d/b.txt"
_cu36_old="$WORKDIR/cu36_old.b3"; _cu36_new="$WORKDIR/cu36_new.b3"
_cu36_out="$WORKDIR/cu36_out"
( cd "$_cu36_d" && b3sum ./*.txt | sort > "$_cu36_old" )
cp "$_cu36_old" "$_cu36_new"
mkdir -p "$_cu36_out"
core_compare "$_cu36_old" "$_cu36_new" "$_cu36_out"
assert_numeric_eq "CU36 NB_MOD=0 bases identiques" 0 "$CORE_COMPARE_NB_MOD"
assert_numeric_eq "CU36 NB_DIS=0 bases identiques" 0 "$CORE_COMPARE_NB_DIS"
assert_numeric_eq "CU36 NB_NOU=0 bases identiques" 0 "$CORE_COMPARE_NB_NOU"

# CU37 : un fichier modifié
_cu37_old_d="$WORKDIR/cu37_old"; _cu37_new_d="$WORKDIR/cu37_new"
mkdir -p "$_cu37_old_d" "$_cu37_new_d"
echo "v1_alpha" > "$_cu37_old_d/alpha.txt"
echo "v1_beta"  > "$_cu37_old_d/beta.txt"
echo "v2_alpha" > "$_cu37_new_d/alpha.txt"   # modifié
echo "v1_beta"  > "$_cu37_new_d/beta.txt"
_cu37_ob="$WORKDIR/cu37_old.b3"; _cu37_nb="$WORKDIR/cu37_new.b3"; _cu37_out="$WORKDIR/cu37_out"
( cd "$_cu37_old_d" && b3sum ./*.txt | sort > "$_cu37_ob" )
( cd "$_cu37_new_d" && b3sum ./*.txt | sort > "$_cu37_nb" )
mkdir -p "$_cu37_out"
core_compare "$_cu37_ob" "$_cu37_nb" "$_cu37_out"
assert_numeric_eq "CU37 NB_MOD=1"          1 "$CORE_COMPARE_NB_MOD"
assert_contains   "CU37 alpha.txt modifié" "alpha.txt" "$(cat "$_cu37_out/modifies.b3")"

# CU38 : plusieurs fichiers modifiés
_cu38_old_d="$WORKDIR/cu38_old"; _cu38_new_d="$WORKDIR/cu38_new"
mkdir -p "$_cu38_old_d" "$_cu38_new_d"
for f in a b c; do
  echo "v1_$f" > "$_cu38_old_d/${f}.txt"
  echo "v2_$f" > "$_cu38_new_d/${f}.txt"
done
_cu38_ob="$WORKDIR/cu38_old.b3"; _cu38_nb="$WORKDIR/cu38_new.b3"; _cu38_out="$WORKDIR/cu38_out"
( cd "$_cu38_old_d" && b3sum ./*.txt | sort > "$_cu38_ob" )
( cd "$_cu38_new_d" && b3sum ./*.txt | sort > "$_cu38_nb" )
mkdir -p "$_cu38_out"
core_compare "$_cu38_ob" "$_cu38_nb" "$_cu38_out"
assert_numeric_eq "CU38 NB_MOD=3" 3 "$CORE_COMPARE_NB_MOD"

# CU39 : un fichier disparu
_cu39_old_d="$WORKDIR/cu39_old"; _cu39_new_d="$WORKDIR/cu39_new"
mkdir -p "$_cu39_old_d" "$_cu39_new_d"
echo "alpha" > "$_cu39_old_d/alpha.txt"
echo "beta"  > "$_cu39_old_d/beta.txt"
echo "beta"  > "$_cu39_new_d/beta.txt"   # alpha disparu
_cu39_ob="$WORKDIR/cu39_old.b3"; _cu39_nb="$WORKDIR/cu39_new.b3"; _cu39_out="$WORKDIR/cu39_out"
( cd "$_cu39_old_d" && b3sum ./*.txt | sort > "$_cu39_ob" )
( cd "$_cu39_new_d" && b3sum ./*.txt | sort > "$_cu39_nb" )
mkdir -p "$_cu39_out"
core_compare "$_cu39_ob" "$_cu39_nb" "$_cu39_out"
assert_numeric_eq "CU39 NB_DIS=1"          1 "$CORE_COMPARE_NB_DIS"
assert_contains   "CU39 alpha.txt disparu" "alpha.txt" "$(cat "$_cu39_out/disparus.txt")"

# CU40 : un fichier nouveau
_cu40_old_d="$WORKDIR/cu40_old"; _cu40_new_d="$WORKDIR/cu40_new"
mkdir -p "$_cu40_old_d" "$_cu40_new_d"
echo "alpha"   > "$_cu40_old_d/alpha.txt"
echo "alpha"   > "$_cu40_new_d/alpha.txt"
echo "epsilon" > "$_cu40_new_d/epsilon.txt"  # nouveau
_cu40_ob="$WORKDIR/cu40_old.b3"; _cu40_nb="$WORKDIR/cu40_new.b3"; _cu40_out="$WORKDIR/cu40_out"
( cd "$_cu40_old_d" && b3sum ./*.txt | sort > "$_cu40_ob" )
( cd "$_cu40_new_d" && b3sum ./*.txt | sort > "$_cu40_nb" )
mkdir -p "$_cu40_out"
core_compare "$_cu40_ob" "$_cu40_nb" "$_cu40_out"
assert_numeric_eq "CU40 NB_NOU=1"            1 "$CORE_COMPARE_NB_NOU"
assert_contains   "CU40 epsilon.txt nouveau" "epsilon.txt" "$(cat "$_cu40_out/nouveaux.txt")"

# CU41 : combinaison modifié + disparu + nouveau
_cu41_old_d="$WORKDIR/cu41_old"; _cu41_new_d="$WORKDIR/cu41_new"
mkdir -p "$_cu41_old_d" "$_cu41_new_d"
echo "v1"     > "$_cu41_old_d/modifie.txt"
echo "v1_dis" > "$_cu41_old_d/disparu.txt"
echo "v2"     > "$_cu41_new_d/modifie.txt"
echo "nvx"    > "$_cu41_new_d/nouveau.txt"
_cu41_ob="$WORKDIR/cu41_old.b3"; _cu41_nb="$WORKDIR/cu41_new.b3"; _cu41_out="$WORKDIR/cu41_out"
( cd "$_cu41_old_d" && b3sum ./*.txt | sort > "$_cu41_ob" )
( cd "$_cu41_new_d" && b3sum ./*.txt | sort > "$_cu41_nb" )
mkdir -p "$_cu41_out"
core_compare "$_cu41_ob" "$_cu41_nb" "$_cu41_out"
assert_numeric_eq "CU41 NB_MOD=1" 1 "$CORE_COMPARE_NB_MOD"
assert_numeric_eq "CU41 NB_DIS=1" 1 "$CORE_COMPARE_NB_DIS"
assert_numeric_eq "CU41 NB_NOU=1" 1 "$CORE_COMPARE_NB_NOU"

# CU42 : chemin avec espace
_cu42_old_d="$WORKDIR/cu42_old"; _cu42_new_d="$WORKDIR/cu42_new"
mkdir -p "$_cu42_old_d" "$_cu42_new_d"
echo "v1" > "$_cu42_old_d/fichier avec espace.txt"
echo "v2" > "$_cu42_new_d/fichier avec espace.txt"
_cu42_ob="$WORKDIR/cu42_old.b3"; _cu42_nb="$WORKDIR/cu42_new.b3"; _cu42_out="$WORKDIR/cu42_out"
( cd "$_cu42_old_d" && b3sum "fichier avec espace.txt" > "$_cu42_ob" )
( cd "$_cu42_new_d" && b3sum "fichier avec espace.txt" > "$_cu42_nb" )
mkdir -p "$_cu42_out"
core_compare "$_cu42_ob" "$_cu42_nb" "$_cu42_out"
assert_contains   "CU42 chemin avec espace dans modifies.b3" "fichier avec espace.txt" "$(cat "$_cu42_out/modifies.b3")"
assert_numeric_eq "CU42 NB_MOD=1"                            1 "$CORE_COMPARE_NB_MOD"

# CU43 : chemin avec &
_cu43_old_d="$WORKDIR/cu43_old"; _cu43_new_d="$WORKDIR/cu43_new"
mkdir -p "$_cu43_old_d" "$_cu43_new_d"
echo "v1" > "$_cu43_old_d/a&b.txt"
echo "v2" > "$_cu43_new_d/a&b.txt"
_cu43_ob="$WORKDIR/cu43_old.b3"; _cu43_nb="$WORKDIR/cu43_new.b3"; _cu43_out="$WORKDIR/cu43_out"
( cd "$_cu43_old_d" && b3sum "a&b.txt" > "$_cu43_ob" )
( cd "$_cu43_new_d" && b3sum "a&b.txt" > "$_cu43_nb" )
mkdir -p "$_cu43_out"
core_compare "$_cu43_ob" "$_cu43_nb" "$_cu43_out"
assert_contains "CU43 chemin avec & dans modifies.b3" "a&b.txt" "$(cat "$_cu43_out/modifies.b3")"

# CU44 : chemin avec < et >
_cu44_old_d="$WORKDIR/cu44_old"; _cu44_new_d="$WORKDIR/cu44_new"
mkdir -p "$_cu44_old_d" "$_cu44_new_d"
echo "v1" > "$_cu44_old_d/<script>.txt"
echo "v2" > "$_cu44_new_d/<script>.txt"
_cu44_ob="$WORKDIR/cu44_old.b3"; _cu44_nb="$WORKDIR/cu44_new.b3"; _cu44_out="$WORKDIR/cu44_out"
( cd "$_cu44_old_d" && b3sum "<script>.txt" > "$_cu44_ob" )
( cd "$_cu44_new_d" && b3sum "<script>.txt" > "$_cu44_nb" )
mkdir -p "$_cu44_out"
core_compare "$_cu44_ob" "$_cu44_nb" "$_cu44_out"
assert_contains "CU44 chemin avec <> dans modifies.b3" "<script>.txt" "$(cat "$_cu44_out/modifies.b3")"
# Pas d'échappement HTML dans .b3 (fichier binaire, pas HTML)
assert_not_contains "CU44 pas d'&lt; dans modifies.b3" "&lt;" "$(cat "$_cu44_out/modifies.b3")"

# CU45 : format de modifies.b3 = "<nouveau_hash>  <chemin>" (format b3sum)
_cu45_line=$(head -1 "$_cu44_out/modifies.b3")
if echo "$_cu45_line" | grep -qE '^[0-9a-f]{64}  .+'; then
  pass "CU45 format modifies.b3 conforme b3sum"
else
  fail "CU45 format modifies.b3 inattendu : $_cu45_line"
fi

# CU46 : variables CORE_COMPARE_NB_* définies après appel
_cu46_out="$WORKDIR/cu46_out"; mkdir -p "$_cu46_out"
core_compare "$_cu36_old" "$_cu36_new" "$_cu46_out"
[ -n "${CORE_COMPARE_NB_MOD+x}" ] && pass "CU46 NB_MOD défini" || fail "CU46 NB_MOD non défini"
[ -n "${CORE_COMPARE_NB_DIS+x}" ] && pass "CU46 NB_DIS défini" || fail "CU46 NB_DIS non défini"
[ -n "${CORE_COMPARE_NB_NOU+x}" ] && pass "CU46 NB_NOU défini" || fail "CU46 NB_NOU non défini"

# CU47 : pas de fichiers tmp résiduels après appel (pattern mktemp standard)
# Note : core_compare nettoie via trap EXIT interne ; on vérifie qu'aucun /tmp/tmp.* récent ne traîne
_tmp_before=$(find /tmp -maxdepth 1 -name 'tmp.*' -newer "$WORKDIR" 2>/dev/null | wc -l)
core_compare "$_cu36_old" "$_cu36_new" "$_cu46_out" 2>/dev/null || true
_tmp_after=$(find /tmp -maxdepth 1 -name 'tmp.*' -newer "$WORKDIR" 2>/dev/null | wc -l)
if [ "$_tmp_after" -le "$_tmp_before" ]; then
  pass "CU47 fichiers tmp nettoyés"
else
  fail "CU47 fichiers tmp résiduels détectés ($((tmp_after - tmp_before)))"
fi

# CU48 : outdir absent → comportement défini (mkdir requis par l'appelant)
_cu48_nonexist="$WORKDIR/cu48_outdir_nonexist"
_cu48_exit=0
core_compare "$_cu36_old" "$_cu36_new" "$_cu48_nonexist" 2>/dev/null || _cu48_exit=$?
# Comportement attendu : échec ou création du dossier selon implémentation.
# On documente ce qui se passe sans imposer un exit code (outdir inexistant est précondition violée).
if [ "$_cu48_exit" -ne 0 ] || [ -d "$_cu48_nonexist" ]; then
  pass "CU48 outdir absent : comportement défini (exit=$_cu48_exit, dir_created=$([ -d "$_cu48_nonexist" ] && echo oui || echo non))"
else
  fail "CU48 outdir absent : comportement indéfini"
fi

# == T_CORE06 - core_make_result_dir =============================================

echo ""
echo "========================================"
echo "  T_CORE06 - core_make_result_dir"
echo "========================================"

_cu_res_root="$WORKDIR/resultats_test"; mkdir -p "$_cu_res_root"

# CU49 : création normale
_cu49_b3="$WORKDIR/hashes.b3"; touch "$_cu49_b3"
_cu49_result=$(core_make_result_dir "$_cu49_b3" "$_cu_res_root")
[ -d "$_cu49_result" ] && pass "CU49 dossier créé" || fail "CU49 dossier absent"
assert_contains "CU49 nom contient 'resultats_hashes'" "resultats_hashes" "$_cu49_result"

# CU50 : anti-collision - dossier existant → suffixe horodaté
# Le dossier résultats_hashes existe déjà depuis CU49
_cu50_result=$(core_make_result_dir "$_cu49_b3" "$_cu_res_root")
if [ "$_cu50_result" != "$_cu49_result" ]; then
  pass "CU50 anti-collision : nouveau dossier créé"
else
  fail "CU50 anti-collision : même dossier retourné (écrasement)"
fi
[ -d "$_cu50_result" ] && pass "CU50 nouveau dossier existe" || fail "CU50 nouveau dossier absent"

# CU51 : deux appels successifs → deux dossiers distincts
_cu51_b3="$WORKDIR/autre.b3"; touch "$_cu51_b3"
_cu51_r1=$(core_make_result_dir "$_cu51_b3" "$_cu_res_root")
sleep 1
_cu51_r2=$(core_make_result_dir "$_cu51_b3" "$_cu_res_root")
if [ "$_cu51_r1" != "$_cu51_r2" ]; then
  pass "CU51 deux appels → deux dossiers distincts"
else
  fail "CU51 deux appels → même dossier (collision)"
fi

# CU52 : nom sans extension .b3
_cu52_b3="$WORKDIR/base"; touch "$_cu52_b3"
_cu52_result=$(core_make_result_dir "$_cu52_b3" "$_cu_res_root")
assert_contains "CU52 nom sans extension → resultats_base" "resultats_base" "$_cu52_result"

# CU53 : nom avec chemin imbriqué → basename only
_cu53_b3="/chemin/vers/hashes.b3"
# On ne crée pas ce fichier - on teste seulement la logique de nommage
_cu53_result=$(core_make_result_dir "$_cu53_b3" "$_cu_res_root")
assert_contains "CU53 chemin imbriqué → resultats_hashes" "resultats_hashes" "$_cu53_result"
# Ne doit pas contenir le chemin complet
assert_not_contains "CU53 pas de chemin absolu dans le nom" "/chemin/vers/" "$_cu53_result"

# == Résultats ===================================================================

echo ""
echo "========================================"
if [ "$FAIL" -eq 0 ]; then
  echo -e "  ${GREEN}$PASS/$TOTAL tests passés${NC}"
else
  echo -e "  ${GREEN}$PASS${NC}/${TOTAL} passés - ${RED}$FAIL échec(s)${NC}"
fi
echo "========================================"
echo ""

[ "$FAIL" -eq 0 ]

--- Fichier : tests/run_tests_pipeline.sh ---
#!/usr/bin/env bash
# run_tests_pipeline.sh - Tests automatisés pour runner.sh + pipeline.json
#
# Couvre : parsing JSON, compute, verify, compare, champ resultats, erreurs
#
# Prérequis : bash >= 4, jq, b3sum
#             runner.sh    à ../
#             integrity.sh à ../src/
# Usage     : cd tests && ./run_tests_pipeline.sh

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
RUNNER="$SCRIPT_DIR/../runner.sh"
INTEGRITY="$SCRIPT_DIR/../src/integrity.sh"
WORKDIR="$(mktemp -d /tmp/integrity-pipeline-test.XXXXXX)"
export RESULTATS_DIR="$WORKDIR/resultats"

GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

PASS=0; FAIL=0; TOTAL=0

pass() { echo -e "${GREEN}  PASS${NC} - $1"; PASS=$((PASS+1)); TOTAL=$((TOTAL+1)); }
fail() { echo -e "${RED}  FAIL${NC} - $1"; FAIL=$((FAIL+1)); TOTAL=$((TOTAL+1)); }

assert_contains() {
    local label="$1" pattern="$2" output="$3"
    if echo "$output" | grep -q "$pattern"; then pass "$label"; else fail "$label (pattern '$pattern' absent)"; fi
}

assert_not_contains() {
    local label="$1" pattern="$2" output="$3"
    if ! echo "$output" | grep -q "$pattern"; then pass "$label"; else fail "$label (pattern '$pattern' présent à tort)"; fi
}

assert_file_exists() {
    local label="$1" file="$2"
    if [ -f "$file" ]; then pass "$label"; else fail "$label (absent : $file)"; fi
}

assert_file_absent() {
    local label="$1" file="$2"
    if [ ! -f "$file" ]; then pass "$label"; else fail "$label (présent à tort : $file)"; fi
}

assert_line_count() {
    local label="$1" expected="$2" file="$3"
    local actual; actual=$(wc -l < "$file")
    if [ "$actual" -eq "$expected" ]; then pass "$label"; else fail "$label (attendu $expected, obtenu $actual)"; fi
}

write_config() {
    local path="$WORKDIR/pipeline.json"
    cat > "$path"
    echo "$path"
}

setup() {
    mkdir -p "$WORKDIR"/{src_a,src_b,bases,resultats}

    echo "alpha content" > "$WORKDIR/src_a/alpha.txt"
    echo "beta content"  > "$WORKDIR/src_a/beta.txt"
    mkdir -p "$WORKDIR/src_a/sub"
    echo "delta content" > "$WORKDIR/src_a/sub/delta.txt"

    echo "gamma content" > "$WORKDIR/src_b/gamma.txt"
    echo "delta content" > "$WORKDIR/src_b/delta.txt"
}

teardown() { rm -rf "$WORKDIR"; }

run_tests() {
    cd "$WORKDIR"

    echo ""
    echo "========================================"
    echo "  runner.sh - suite de tests"
    echo "  Workdir : $WORKDIR"
    echo "========================================"
    echo ""

    echo "TP00 - Permissions integrity.sh"
    if [ -x "$INTEGRITY" ]; then pass "integrity.sh est exécutable"; else fail "integrity.sh non exécutable (chmod +x requis)"; fi
    echo ""

    # == TP01 : JSON invalide ==================================================
    echo "TP01 - JSON invalide : erreur propre sans stacktrace jq"
    local cfg_invalid="$WORKDIR/invalid.json"
    echo "{ pipeline: [ BROKEN" > "$cfg_invalid"
    local out_tp01; out_tp01=$(bash "$RUNNER" "$cfg_invalid" 2>&1 || true)
    assert_contains     "ERREUR signalée"         "ERREUR"      "$out_tp01"
    assert_not_contains "pas de stacktrace jq"    "parse error" "$out_tp01"
    echo ""

    # == TP02 : .pipeline absent ===============================================
    echo "TP02 - .pipeline absent"
    local cfg_no_pipeline
    cfg_no_pipeline=$(write_config <<'EOF'
{ "config": [] }
EOF
)
    local out_tp02; out_tp02=$(bash "$RUNNER" "$cfg_no_pipeline" 2>&1 || true)
    assert_contains "ERREUR si .pipeline absent" "ERREUR" "$out_tp02"
    echo ""

    # == TP03 : champ manquant =================================================
    echo "TP03 - Champ 'nom' manquant dans compute"
    local cfg_missing
    cfg_missing=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compute", "source": "$WORKDIR/src_a", "bases": "$WORKDIR/bases" }
    ]
}
EOF
)
    local out_tp03; out_tp03=$(bash "$RUNNER" "$cfg_missing" 2>&1 || true)
    assert_contains "ERREUR signalée"       "ERREUR" "$out_tp03"
    assert_contains "champ 'nom' mentionné" "nom"    "$out_tp03"
    echo ""

    # == TP04 : opération inconnue =============================================
    echo "TP04 - Opération inconnue"
    local cfg_unknown
    cfg_unknown=$(write_config <<'EOF'
{ "pipeline": [ { "op": "migrate", "source": "/tmp" } ] }
EOF
)
    local out_tp04; out_tp04=$(bash "$RUNNER" "$cfg_unknown" 2>&1 || true)
    assert_contains "ERREUR signalée"           "ERREUR"   "$out_tp04"
    assert_contains "nom de l'op dans l'erreur" "migrate"  "$out_tp04"
    echo ""

    # == TP05 : compute - chemins relatifs =====================================
    echo "TP05 - Compute : cd correct, chemins relatifs dans la base"
    local cfg_compute
    cfg_compute=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compute", "source": "$WORKDIR/src_a", "bases": "$WORKDIR/bases", "nom": "hashes_a.b3" }
    ]
}
EOF
)
    bash "$RUNNER" "$cfg_compute" >/dev/null 2>&1
    assert_file_exists "base hashes_a.b3 créée" "$WORKDIR/bases/hashes_a.b3"
    local first_path; first_path=$(awk '{print $2}' "$WORKDIR/bases/hashes_a.b3" | head -1)
    assert_contains     "chemin relatif (./) dans base"    "./"       "$first_path"
    assert_not_contains "pas de chemin absolu dans base"   "$WORKDIR" "$first_path"
    assert_line_count   "3 fichiers indexés"               3          "$WORKDIR/bases/hashes_a.b3"
    echo ""

    # == TP06 : compute - source absente ======================================
    echo "TP06 - Compute : source absente → erreur"
    local cfg_absent
    cfg_absent=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compute", "source": "$WORKDIR/inexistant", "bases": "$WORKDIR/bases", "nom": "ko.b3" }
    ]
}
EOF
)
    local out_tp06; out_tp06=$(bash "$RUNNER" "$cfg_absent" 2>&1 || true)
    assert_contains    "ERREUR signalée"              "ERREUR"  "$out_tp06"
    assert_file_absent "pas de base créée si source KO" "$WORKDIR/bases/ko.b3"
    echo ""

    # == TP07 : verify - OK ===================================================
    echo "TP07 - Verify : répertoire de travail correct, vérification OK"
    local cfg_verify
    cfg_verify=$(write_config <<EOF
{
    "pipeline": [
        { "op": "verify", "source": "$WORKDIR/src_a", "base": "$WORKDIR/bases/hashes_a.b3" }
    ]
}
EOF
)
    local out_tp07; out_tp07=$(bash "$RUNNER" "$cfg_verify" 2>&1 || true)
    assert_contains     "verify OK"     "OK"     "$out_tp07"
    assert_not_contains "aucun FAILED"  "FAILED" "$out_tp07"
    local outdir_tp07; outdir_tp07=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | sort | tail -1)
    assert_file_exists  "recap.txt produit" "${outdir_tp07}/recap.txt"
    echo ""

    # == TP08 : verify - corruption ===========================================
    echo "TP08 - Verify : corruption détectée"
    echo "contenu corrompu" > "$WORKDIR/src_a/alpha.txt"
    local out_tp08; out_tp08=$(bash "$RUNNER" "$cfg_verify" 2>&1 || true)
    assert_contains "ECHEC détecté" "ECHEC" "$out_tp08"
    echo "alpha content"   > "$WORKDIR/src_a/alpha.txt"
    echo ""

    # == TP09 : verify - base absente =========================================
    echo "TP09 - Verify : base .b3 absente → erreur"
    local cfg_verify_bad
    cfg_verify_bad=$(write_config <<EOF
{
    "pipeline": [
        { "op": "verify", "source": "$WORKDIR/src_a", "base": "$WORKDIR/bases/fantome.b3" }
    ]
}
EOF
)
    local out_tp09; out_tp09=$(bash "$RUNNER" "$cfg_verify_bad" 2>&1 || true)
    assert_contains "ERREUR si base absente" "ERREUR" "$out_tp09"
    echo ""

    # == TP10 : compare - résultats produits (RESULTATS_DIR par défaut) ========
    echo "TP10 - Compare : fichiers de résultats produits (sans champ resultats)"
    local cfg_compute_b
    cfg_compute_b=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compute", "source": "$WORKDIR/src_b", "bases": "$WORKDIR/bases", "nom": "hashes_b.b3" }
    ]
}
EOF
)
    bash "$RUNNER" "$cfg_compute_b" >/dev/null 2>&1

    local cfg_compare
    cfg_compare=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compare", "base_a": "$WORKDIR/bases/hashes_a.b3", "base_b": "$WORKDIR/bases/hashes_b.b3" }
    ]
}
EOF
)
    bash "$RUNNER" "$cfg_compare" >/dev/null 2>&1
    local outdir_tp10; outdir_tp10=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | sort | tail -1)
    assert_file_exists "recap.txt"    "${outdir_tp10}/recap.txt"
    assert_file_exists "modifies.b3"  "${outdir_tp10}/modifies.b3"
    assert_file_exists "disparus.txt" "${outdir_tp10}/disparus.txt"
    assert_file_exists "nouveaux.txt" "${outdir_tp10}/nouveaux.txt"
    assert_file_exists "report.html"  "${outdir_tp10}/report.html"
    echo ""

    # == TP10b : compare - champ resultats personnalisé =======================
    echo "TP10b - Compare : champ 'resultats' personnalisé dans pipeline.json"
    local custom_dir="$WORKDIR/mon_rapport_custom"
    local cfg_compare_custom
    cfg_compare_custom=$(write_config <<EOF
{
    "pipeline": [
        {
            "op":       "compare",
            "base_a":   "$WORKDIR/bases/hashes_a.b3",
            "base_b":   "$WORKDIR/bases/hashes_b.b3",
            "resultats": "$custom_dir"
        }
    ]
}
EOF
)
    bash "$RUNNER" "$cfg_compare_custom" >/dev/null 2>&1
    local outdir_custom; outdir_custom=$(find "${custom_dir}" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | sort | tail -1)
    assert_file_exists "rapport dans dossier custom"            "${outdir_custom}/recap.txt"
    assert_file_exists "report.html dans dossier custom"        "${outdir_custom}/report.html"
    # Vérifier que le dossier par défaut n'a PAS reçu ce résultat
    local nb_before; nb_before=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | wc -l)
    bash "$RUNNER" "$cfg_compare_custom" >/dev/null 2>&1
    local nb_after; nb_after=$(find "${RESULTATS_DIR}" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | wc -l)
    if [ "$nb_before" -eq "$nb_after" ]; then
        pass "champ resultats isolé : RESULTATS_DIR par défaut non pollué"
    else
        fail "champ resultats isolé : RESULTATS_DIR par défaut non pollué"
    fi
    echo ""

    # == TP11 : compare - base_a absente ======================================
    echo "TP11 - Compare : base_a absente → erreur"
    local cfg_compare_bad
    cfg_compare_bad=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compare", "base_a": "$WORKDIR/bases/fantome.b3", "base_b": "$WORKDIR/bases/hashes_b.b3" }
    ]
}
EOF
)
    local out_tp11; out_tp11=$(bash "$RUNNER" "$cfg_compare_bad" 2>&1 || true)
    assert_contains "ERREUR si base_a absente" "ERREUR" "$out_tp11"
    echo ""

    # == TP12 : pipeline complet ===============================================
    echo "TP12 - Pipeline complet : compute × 2 + verify + compare"
    rm -f "$WORKDIR/bases/hashes_a.b3" "$WORKDIR/bases/hashes_b.b3"
    local cfg_full
    cfg_full=$(write_config <<EOF
{
    "pipeline": [
        { "op": "compute", "source": "$WORKDIR/src_a", "bases": "$WORKDIR/bases", "nom": "hashes_a.b3" },
        { "op": "compute", "source": "$WORKDIR/src_b", "bases": "$WORKDIR/bases", "nom": "hashes_b.b3" },
        { "op": "verify",  "source": "$WORKDIR/src_a", "base":  "$WORKDIR/bases/hashes_a.b3" },
        { "op": "compare", "base_a": "$WORKDIR/bases/hashes_a.b3", "base_b": "$WORKDIR/bases/hashes_b.b3",
          "resultats": "$WORKDIR/resultats_pipeline" }
    ]
}
EOF
)
    local out_tp12; out_tp12=$(bash "$RUNNER" "$cfg_full" 2>&1 || true)
    assert_contains     "COMPUTE mentionné"     "COMPUTE" "$out_tp12"
    assert_contains     "VERIFY mentionné"      "VERIFY"  "$out_tp12"
    assert_contains     "COMPARE mentionné"     "COMPARE" "$out_tp12"
    assert_file_exists  "hashes_a.b3 créée"     "$WORKDIR/bases/hashes_a.b3"
    assert_file_exists  "hashes_b.b3 créée"     "$WORKDIR/bases/hashes_b.b3"
    assert_not_contains "pas d'ERREUR"          "ERREUR"  "$out_tp12"
    local outdir_tp12; outdir_tp12=$(find "${WORKDIR}/resultats_pipeline" -maxdepth 1 -type d -name "resultats_hashes_a*" 2>/dev/null | sort | tail -1)
    assert_file_exists  "report.html pipeline complet" "${outdir_tp12}/report.html"
    echo ""
}

# == Main ======================================================================

for dep in jq b3sum; do
    command -v "$dep" &>/dev/null || { echo -e "${RED}ERREUR${NC} : $dep non trouvé."; exit 1; }
done

[ -f "$RUNNER" ]    || { echo -e "${RED}ERREUR${NC} : runner.sh introuvable : $RUNNER";      exit 1; }
[ -f "$INTEGRITY" ] || { echo -e "${RED}ERREUR${NC} : src/integrity.sh introuvable : $INTEGRITY"; exit 1; }

setup
run_tests
teardown

echo "========================================"
if [ "$FAIL" -eq 0 ]; then
    echo -e "  ${GREEN}$PASS/$TOTAL tests passés${NC}"
else
    echo -e "  ${GREEN}$PASS${NC}/${TOTAL} passés - ${RED}$FAIL échec(s)${NC}"
fi
echo "========================================"
echo ""

[ "$FAIL" -eq 0 ]

--- Fichier : .github/workflows/ci.yml ---
name: CI

on:
  push:
    branches: ["**"]
  pull_request:
    branches: ["**"]

jobs:
  # ===========================================================================
  # Tests fonctionnels + unitaires
  # ===========================================================================
  tests:
    name: Tests (${{ matrix.os }})
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, ubuntu-24.04]

    steps:
      - uses: actions/checkout@v4

      - name: Installer les dépendances
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y b3sum jq shellcheck

      - name: Vérifier les prérequis
        run: |
          b3sum --version
          jq --version
          shellcheck --version
          bash --version

      - name: T00-T20 - integrity.sh (run_tests.sh)
        run: |
          chmod +x tests/run_tests.sh
          cd tests && ./run_tests.sh

      - name: TP01-TP12 - pipeline (run_tests_pipeline.sh)
        run: |
          chmod +x tests/run_tests_pipeline.sh
          cd tests && ./run_tests_pipeline.sh

      - name: CU01-CU53 - tests unitaires core.sh (run_tests_core.sh)
        run: |
          chmod +x tests/run_tests_core.sh
          cd tests && ./run_tests_core.sh

      - name: ShellCheck - tous les scripts
        run: |
          shellcheck src/integrity.sh
          shellcheck runner.sh
          shellcheck src/lib/core.sh
          shellcheck src/lib/ui.sh
          shellcheck src/lib/report.sh
          shellcheck src/lib/results.sh
          shellcheck docker/entrypoint.sh
          shellcheck tests/run_tests.sh
          shellcheck tests/run_tests_pipeline.sh
          shellcheck tests/run_tests_core.sh

      - name: Upload artefacts de test
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.os }}
          path: /tmp/integrity-test*/
          retention-days: 7

  # ===========================================================================
  # Tests Docker
  # ===========================================================================
  docker:
    name: Docker build & smoke tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Build de l'image
        run: docker build -t hash_tool .

      - name: Smoke test - version
        run: docker run --rm hash_tool version

      - name: Smoke test - help
        run: docker run --rm hash_tool help

      - name: Smoke test - check-env
        run: docker run --rm hash_tool check-env

      - name: Smoke test - compute via volume
        run: |
          mkdir -p /tmp/testdata /tmp/testbases
          echo "contenu alpha" > /tmp/testdata/alpha.txt
          echo "contenu beta"  > /tmp/testdata/beta.txt
          docker run --rm \
            -v /tmp/testdata:/data:ro \
            -v /tmp/testbases:/bases \
            hash_tool compute /data /bases/test.b3
          # Vérifie que le fichier .b3 est produit et non vide
          [ -s /tmp/testbases/test.b3 ] || (echo "ERREUR : test.b3 vide ou absent" && exit 1)
          echo "test.b3 produit : $(wc -l < /tmp/testbases/test.b3) ligne(s)"

      - name: Smoke test - verify via volume
        run: |
          docker run --rm \
            -v /tmp/testdata:/data:ro \
            -v /tmp/testbases:/bases:ro \
            -v /tmp/testresultats:/resultats \
            -e RESULTATS_DIR=/resultats \
            hash_tool verify /bases/test.b3 /data

      - name: Entrypoint - commande inconnue → exit non-zéro
        run: |
          docker run --rm hash_tool commande_inexistante && exit 1 || true


--- Fichier : reports/template.html ---
<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{{TITLE}}</title>
  <style>
    :root {
      --bg: #ffffff;
      --text: #1a1a1a;
      --text-dim: #666666;
      --border: #e0e0e0;
      --accent-ok: #2e7d32;
      --accent-err: #d32f2f;
      --section-gap: 40px;
      --mono: 'JetBrains Mono', monospace;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      background-color: var(--bg);
      color: var(--text);
      font-family: system-ui, -apple-system, sans-serif;
      font-size: 15px;
      line-height: 1.5;
      padding: 40px 20px;
    }
    .container { max-width: 850px; margin: 0 auto; }
    header { border-bottom: 2px solid var(--text); margin-bottom: var(--section-gap); padding-bottom: 20px; }
    h1 { font-size: 1.5rem; text-transform: uppercase; letter-spacing: 0.05em; }
    .comparison-path { font-family: var(--mono); font-size: 0.85rem; color: var(--text-dim); margin-top: 10px; }
    .summary-box { background: #f9f9f9; border: 1px solid var(--border); padding: 20px; margin-bottom: var(--section-gap); }
    .summary-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 20px; }
    .stat-item { display: flex; flex-direction: column; }
    .stat-label { font-size: 0.75rem; text-transform: uppercase; color: var(--text-dim); font-weight: 600; }
    .stat-value { font-family: var(--mono); font-size: 1.3rem; font-weight: 500; }
    section { margin-bottom: var(--section-gap); }
    section h2 { font-size: 1.1rem; text-transform: uppercase; border-left: 4px solid var(--text); padding-left: 15px; margin-bottom: 20px; }
    .info-list { display: grid; grid-template-columns: 200px 1fr; gap: 10px 20px; }
    .info-label { color: var(--text-dim); font-size: 0.85rem; }
    .info-value { font-family: var(--mono); font-size: 0.9rem; word-break: break-all; }
    .file-list { list-style: none; border-top: 1px solid #f0f0f0; }
    .file-list li { padding: 8px 0; border-bottom: 1px solid #f0f0f0; font-family: var(--mono); font-size: 0.85rem; }
    .empty-msg { font-style: italic; color: var(--text-dim); }
    footer { margin-top: 60px; padding-top: 20px; border-top: 1px solid var(--border); font-size: 0.75rem; color: var(--text-dim); text-align: center; font-family: var(--mono); }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>{{TITLE}}</h1>
      <div class="comparison-path">{{PATHS}}</div>
    </header>

    <div class="summary-box">
      <div class="summary-grid">
        <div class="stat-item">
          <span class="stat-label">Statut</span>
          <span class="stat-value" style="color: {{STATUS_COLOR}}">{{STATUS_TEXT}}</span>
        </div>
        <div class="stat-item">
          <span class="stat-label">Date d'analyse</span>
          <span class="stat-value" style="font-size: 1rem">{{DATE}}</span>
        </div>
      </div>
    </div>

    <section>
      <h2>Métadonnées</h2>
      <div class="info-list">
        {{METADATA_ROWS}}
      </div>
    </section>

    <section>
      <h2>Fichiers modifiés</h2>
      {{LIST_MODIFIED}}
    </section>

    <section>
      <h2>Fichiers disparus</h2>
      {{LIST_DELETED}}
    </section>

    <section>
      <h2>Nouveaux fichiers</h2>
      {{LIST_NEW}}
    </section>

    <footer>
      Généré par hash-tool · {{DATE}}
    </footer>
  </div>
</body>
</html>

--- Fichier : src/integrity.sh ---
#!/usr/bin/env bash
# integrity.sh - Vérification d'intégrité par hachage BLAKE3
#
# Point d'entrée CLI interne. Orchestre les modules :
#   src/lib/core.sh    - logique métier (hachage, vérification, comparaison, sidecar)
#   src/lib/ui.sh      - interface terminal (affichage, ETA, progression)
#   src/lib/results.sh - écriture des fichiers de résultats
#   src/lib/report.sh  - génération des rapports HTML
#
# Usage :
#   ./integrity.sh [--quiet] compute <dossier> <base.b3> [commentaire_sidecar]
#   ./integrity.sh [--quiet] verify  <base.b3> [dossier]
#   ./integrity.sh [--quiet] compare <ancienne.b3> <nouvelle.b3>
#
# Options :
#   --quiet   Supprime toute sortie terminal. Écrit uniquement dans les
#             fichiers de résultats. Exit code propagé sans modification.
#
# Sidecar :
#   compute génère automatiquement <base.b3>.meta.json si jq est disponible.
#   Le troisième argument optionnel de compute est un commentaire libre.
#   verify et compare affichent le sidecar si présent (sauf --quiet).
#
# Dépendances : b3sum, bash >= 4, find, sort, awk, comm, join, stat, du, mktemp
#               jq (optionnel - requis pour la génération du sidecar)
#
# Exit codes :
#   0 - succès (voir contrat de chaque mode dans src/lib/core.sh)
#   1 - erreur (argument manquant, fichier introuvable, corruption détectée)

set -euo pipefail

# == Version ====================================================================

INTEGRITY_VERSION="2.0.0"

# == Prérequis bash =============================================================

(( BASH_VERSINFO[0] >= 4 )) || {
  echo "ERREUR : bash >= 4 requis (actuel : $BASH_VERSION)" >&2
  exit 1
}

# == Résolution des chemins =====================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# == Chargement des modules =====================================================

for _module in ui core results report; do
  _path="$SCRIPT_DIR/lib/${_module}.sh"
  [ -f "$_path" ] || { echo "ERREUR : module introuvable : $_path" >&2; exit 1; }
  # shellcheck source=/dev/null
  source "$_path"
done
unset _module _path

# == Parsing des arguments ======================================================

QUIET=0
ARGS=()

for arg in "$@"; do
  case "$arg" in
    --quiet) QUIET=1 ;;
    *)       ARGS+=("$arg") ;;
  esac
done

MODE="${ARGS[0]:-}"
ARG2="${ARGS[1]:-}"
ARG3="${ARGS[2]:-}"
ARG4="${ARGS[3]:-}"   # commentaire sidecar optionnel pour compute

# == Configuration ==============================================================

# Dossier racine des résultats. Peut être surchargé par variable d'environnement.
# runner.sh surcharge cette valeur via export pour isoler les runs de pipeline.
RESULTATS_DIR="${RESULTATS_DIR:-${HOME}/integrity_resultats}"

# == Handlers des modes =========================================================

_run_compute() {
  local target="$ARG2"
  local hashfile="$ARG3"
  local sidecar_comment="${ARG4:-}"

  [ -n "$target"   ] || die "compute : dossier cible manquant.\nUsage : $0 compute <dossier> <base.b3> [commentaire]"
  [ -n "$hashfile" ] || die "compute : fichier de sortie .b3 manquant.\nUsage : $0 compute <dossier> <base.b3> [commentaire]"
  [ ! -d "$hashfile" ] || die "compute : '$hashfile' est un dossier. Le fichier .b3 de sortie doit être un chemin de fichier."

  core_assert_target_valid "$target"

  # Utilise ui_progress_callback uniquement si QUIET == 0
  local callback=""
  (( QUIET )) || callback="ui_progress_callback"

  core_compute "$target" "$hashfile" "$callback"
  ui_progress_clear

  say "Base enregistrée : $hashfile ($(wc -l < "$hashfile") fichiers)"

  # Sidecar : généré si jq est disponible
  if command -v jq &>/dev/null; then
    core_sidecar_write "$hashfile" "$target" "$sidecar_comment" "$INTEGRITY_VERSION"
    say "Sidecar : ${hashfile}.meta.json"
  fi
}

_run_verify() {
  local b3file="$ARG2"
  local workdir="${ARG3:-}"

  [ -n "$b3file" ] || die "verify : fichier .b3 manquant.\nUsage : $0 verify <base.b3> [dossier]"

  core_assert_b3_valid "$b3file" "base"

  # Résolution du chemin absolu AVANT le cd : un chemin relatif deviendrait
  # invalide après changement de répertoire
  local hashfile_abs
  hashfile_abs="$(cd "$(dirname "$b3file")" && pwd)/$(basename "$b3file")"

  # Affichage du sidecar avant le cd (chemin encore valide)
  (( QUIET )) || core_sidecar_read "$hashfile_abs"

  if [ -n "$workdir" ]; then
    [ -d "$workdir" ] || die "verify : '$workdir' n'est pas un dossier valide."
    cd "$workdir"
  fi

  local outdir
  outdir=$(core_make_result_dir "$hashfile_abs" "$RESULTATS_DIR")

  # core_verify positionne les variables CORE_VERIFY_* dans le scope courant
  local exit_code=0
  core_verify "$hashfile_abs" || exit_code=$?

  results_write_verify \
    "$outdir" "$hashfile_abs" \
    "$CORE_VERIFY_STATUS" "$CORE_VERIFY_NB_OK" "$CORE_VERIFY_NB_FAIL" \
    "$CORE_VERIFY_LINES_FAIL" "$CORE_VERIFY_LINES_ERR"

  ui_show_verify_result \
    "$CORE_VERIFY_STATUS" "$CORE_VERIFY_NB_OK" "$CORE_VERIFY_NB_FAIL" \
    "$CORE_VERIFY_LINES_FAIL" "$CORE_VERIFY_LINES_ERR" \
    "$outdir"

  return $exit_code
}

_run_compare() {
  local old="$ARG2"
  local new="$ARG3"

  [ -n "$old" ] || die "compare : fichier ancienne base manquant.\nUsage : $0 compare <ancienne.b3> <nouvelle.b3>"
  [ -n "$new" ] || die "compare : fichier nouvelle base manquant.\nUsage : $0 compare <ancienne.b3> <nouvelle.b3>"

  core_assert_b3_valid "$old" "ancienne base"
  core_assert_b3_valid "$new" "nouvelle base"

  # Affichage des sidecars avant toute opération
  if (( ! QUIET )); then
    core_sidecar_read "$old"
    core_sidecar_read "$new"
  fi

  local outdir
  outdir=$(core_make_result_dir "$old" "$RESULTATS_DIR")

  # core_compare positionne CORE_COMPARE_NB_* dans le scope courant
  core_compare "$old" "$new" "$outdir"

  results_write_compare \
    "$outdir" "$old" "$new" \
    "$CORE_COMPARE_NB_MOD" "$CORE_COMPARE_NB_DIS" "$CORE_COMPARE_NB_NOU"

  generate_compare_html \
    "$old" "$new" \
    "$CORE_COMPARE_NB_MOD" "$CORE_COMPARE_NB_DIS" "$CORE_COMPARE_NB_NOU" \
    "${outdir}/modifies.b3" "${outdir}/disparus.txt" "${outdir}/nouveaux.txt" \
    "${outdir}/report.html"

  ui_show_compare_result \
    "$CORE_COMPARE_NB_MOD" "$CORE_COMPARE_NB_DIS" "$CORE_COMPARE_NB_NOU" \
    "$outdir"
}

# == Dispatch ===================================================================

case "$MODE" in
  compute) _run_compute ;;
  verify)  _run_verify  ;;
  compare) _run_compare ;;
  *)
    cat <<EOF
Usage :
  $0 [--quiet] compute <dossier> <base.b3> [commentaire]
  $0 [--quiet] verify  <base.b3> [dossier]
  $0 [--quiet] compare <ancienne.b3> <nouvelle.b3>

Options :
  --quiet      Silencieux : écrit uniquement dans les fichiers de résultats.

Arguments optionnels :
  [commentaire]  Texte libre stocké dans le sidecar <base.b3>.meta.json (compute uniquement).
                 Nécessite jq.

Note :
  Pour l'interface complète (list, diff, stats, check-env, version, pipeline),
  utiliser hash-tool à la racine du projet.
EOF
    exit 1
    ;;
esac

--- Fichier : src/lib/core.sh ---
#!/usr/bin/env bash
# src/lib/core.sh - Logique métier BLAKE3 : hachage, vérification, comparaison, sidecar
#
# Ce module contient uniquement la logique métier. Il ne produit aucune
# sortie terminal directement - toute communication avec l'utilisateur
# est déléguée à src/lib/ui.sh via les codes de retour et les variables
# de sortie déclarées dans les contrats ci-dessous.
#
# Sourcé par src/integrity.sh. Ne pas exécuter directement.
#
# == Dépendances ================================================================
#   b3sum, find, sort, awk, join, comm, mktemp, stat, du
#   jq (optionnel - requis pour core_sidecar_write et core_sidecar_read)
#
# == Invariants globaux =========================================================
#   - Toutes les fonctions supposent bash >= 4 (vérifié par integrity.sh)
#   - Les chemins dans les bases .b3 sont toujours RELATIFS (jamais absolus)
#   - Le format .b3 est celui natif de b3sum : "<hash64>  <chemin>" (2 espaces)
#   - L'encodage supposé est UTF-8 ; les noms non UTF-8 sont traités comme des
#     séquences d'octets opaques (find -print0 / mapfile -d '' garantissent
#     l'absence d'interprétation)
#   - La locale n'affecte pas le tri : sort utilise l'ordre binaire (LC_ALL=C
#     doit être positionné par l'appelant si nécessaire pour reproductibilité)

# == Validation =================================================================

# core_assert_b3_valid <fichier> [label]
#
# Contrat d'entrée :
#   $1 - chemin vers un fichier .b3 à valider
#   $2 - label optionnel pour les messages d'erreur (défaut : $1)
#
# Contrat de sortie :
#   exit 0  - fichier valide : existe, est un fichier régulier, non vide,
#             contient au moins une ligne au format "<hash64>  <chemin>"
#   exit 1  - fichier invalide ; appelle die() avec message explicite
#
# Effets de bord : aucun
core_assert_b3_valid() {
  local file="$1"
  local label="${2:-$1}"

  [ -e "$file" ] || die "$label : fichier introuvable."
  [ -f "$file" ] || die "$label : est un dossier, pas un fichier .b3."
  [ -s "$file" ] || die "$label : fichier vide - aucun hash à traiter."

  local valid_lines
  valid_lines=$(grep -c -E '^[0-9a-f]{64}  .+' "$file" || true)
  [ "$valid_lines" -gt 0 ] || die "$label : format invalide - aucune ligne au format b3sum détectée."

  local total_lines
  total_lines=$(wc -l < "$file")
  if [ "$total_lines" -gt "$valid_lines" ]; then
    die "$label : fichier corrompu - $((total_lines - valid_lines)) ligne(s) sur $total_lines ne respectent pas le format b3sum."
  fi
}

# core_assert_target_valid <dossier>
#
# Contrat d'entrée :
#   $1 - chemin vers un dossier à indexer
#
# Contrat de sortie :
#   exit 0  - dossier valide : existe, est un dossier, contient au moins un fichier régulier
#   exit 1  - invalide ; appelle die() avec message explicite
#
# Effets de bord : aucun
core_assert_target_valid() {
  local dir="$1"

  [ -e "$dir" ] || die "Dossier cible introuvable : $dir"
  [ -d "$dir" ] || die "Le chemin cible n'est pas un dossier : $dir"

  local nb_files
  nb_files=$(find "$dir" -type f -print0 | grep -zc '' || echo 0)
  (( nb_files > 0 )) || die "Le dossier $dir ne contient aucun fichier régulier - rien à hacher."
}

# == Utilitaires internes ========================================================

# _core_file_size <fichier>
#
# Contrat de sortie :
#   stdout - taille du fichier en octets (entier)
#   Portable : GNU stat (-c%s) avec fallback BSD stat (-f%z)
#
# Effets de bord : aucun
_core_file_size() {
  local f="$1"
  if stat -c%s "$f" 2>/dev/null; then
    return
  fi
  stat -f%z "$f"
}

# == Hachage ====================================================================

# core_compute <dossier> <fichier_sortie> [callback_progression]
#
# Contrat d'entrée :
#   $1 - dossier cible (chemin relatif RECOMMANDÉ pour portabilité des bases)
#   $2 - chemin du fichier .b3 de sortie (créé ou écrasé)
#   $3 - (optionnel) nom d'une fonction de callback appelée après chaque fichier
#         Signature callback : callback <i> <total_files> <bytes_done> <total_bytes> <eta_seconds>
#         Passer "" ou omettre pour désactiver la progression
#
# Contrat de sortie :
#   exit 0       - base calculée avec succès
#   exit 1       - erreur (propagée depuis b3sum ou find)
#   $2 (fichier) - contient N lignes "<hash64>  <chemin>", triées par chemin,
#                  sans artefact terminal (ETA, \r, etc.)
#
# Invariants garantis :
#   - Les chemins dans $2 sont identiques à ceux vus par find depuis $1
#   - L'ordre est déterministe (sort -z sur les chemins)
#   - Aucune ligne ETA ou de progression ne peut être écrite dans $2 :
#     b3sum est appelé par fichier individuel, la progression est gérée
#     par le callback, pas par redirection
#
# Effets de bord :
#   - Crée ou écrase $2
#   - Appelle $3 si fourni (effets de bord dépendants du callback)
core_compute() {
  local target="$1"
  local hashfile="$2"
  local callback="${3:-}"

  local -a files
  mapfile -d '' files < <(find "$target" -type f -print0 | sort -z)

  local total_files=${#files[@]}
  local total_bytes
  total_bytes=$(du -sb "$target" | awk '{print $1}')

  local bytes_done=0
  local t_start
  t_start=$(date +%s)

  local i=0
  for file in "${files[@]}"; do
    b3sum "$file" >> "$hashfile"

    local fsize
    fsize=$(_core_file_size "$file")
    # Fichier de taille zéro : bytes_done inchangé, ETA non calculée pour ce fichier
    if (( fsize > 0 )); then
      bytes_done=$(( bytes_done + fsize ))
    fi
    i=$(( i + 1 ))

    if [ -n "$callback" ]; then
      local t_now elapsed eta_seconds=0
      t_now=$(date +%s)
      elapsed=$(( t_now - t_start ))
      if (( bytes_done > 0 && elapsed > 0 )); then
        # shellcheck disable=SC2034  # remaining : réservé pour usage futur (affichage temps restant)
        local speed remaining
        speed=$(( bytes_done / elapsed ))
        if (( speed > 0 )); then
          eta_seconds=$(( (total_bytes - bytes_done) / speed ))
        else
          eta_seconds=0
        fi
      fi
      "$callback" "$i" "$total_files" "$bytes_done" "$total_bytes" "$eta_seconds"
    fi
  done
}

# == Vérification ===============================================================

# core_verify <fichier_b3>
#
# Contrat d'entrée :
#   $1 - chemin absolu vers un fichier .b3 valide (validé par core_assert_b3_valid)
#         Le répertoire de travail courant DOIT être celui depuis lequel compute
#         a été exécuté (les chemins dans .b3 sont relatifs à ce répertoire)
#
# Contrat de sortie :
#   exit 0  - tous les fichiers intègres
#   exit 1  - au moins un FAILED ou erreur b3sum
#   CORE_VERIFY_RAW        - sortie brute de b3sum --check
#   CORE_VERIFY_LINES_OK   - lignes "chemin: OK"
#   CORE_VERIFY_LINES_FAIL - lignes "chemin: FAILED"
#   CORE_VERIFY_LINES_ERR  - lignes d'erreur b3sum non liées aux hashes
#   CORE_VERIFY_NB_OK      - entier : nombre de fichiers OK
#   CORE_VERIFY_NB_FAIL    - entier : nombre de fichiers FAILED
#   CORE_VERIFY_STATUS     - "OK" | "ECHEC" | "ERREUR"
#
# Effets de bord :
#   - Positionne les variables CORE_VERIFY_* dans le scope de l'appelant
#     (les variables doivent être déclarées locales dans l'appelant si isolation requise)
core_verify() {
  local hashfile="$1"

  local raw exit_code
  raw=$(b3sum --check "$hashfile" 2>&1) && exit_code=0 || exit_code=$?

  # Variables de sortie lues par l'appelant (integrity.sh) - pas des variables locales
  # shellcheck disable=SC2034
  CORE_VERIFY_RAW="$raw"
  # shellcheck disable=SC2034
  CORE_VERIFY_LINES_OK=$(echo    "$raw" | grep ': OK$'    || true)
  # shellcheck disable=SC2034
  CORE_VERIFY_LINES_FAIL=$(echo  "$raw" | grep ': FAILED' || true)
  # shellcheck disable=SC2034
  CORE_VERIFY_LINES_ERR=$(echo   "$raw" | grep -Ev ': (OK|FAILED)' | grep -v '^$' || true)

  if [ -n "$CORE_VERIFY_LINES_OK" ]; then
    # shellcheck disable=SC2034
    CORE_VERIFY_NB_OK=$(echo "$CORE_VERIFY_LINES_OK" | grep -c '^')
  else
    # shellcheck disable=SC2034
    CORE_VERIFY_NB_OK=0
  fi

  if [ -n "$CORE_VERIFY_LINES_FAIL" ]; then
    # shellcheck disable=SC2034
    CORE_VERIFY_NB_FAIL=$(echo "$CORE_VERIFY_LINES_FAIL" | grep -c '^')
  else
    # shellcheck disable=SC2034
    CORE_VERIFY_NB_FAIL=0
  fi

  if [ -n "$CORE_VERIFY_LINES_ERR" ]; then
    # shellcheck disable=SC2034
    CORE_VERIFY_STATUS="ERREUR"
  elif (( CORE_VERIFY_NB_FAIL > 0 )); then
    # shellcheck disable=SC2034
    CORE_VERIFY_STATUS="ECHEC"
  else
    # shellcheck disable=SC2034
    CORE_VERIFY_STATUS="OK"
  fi

  return $exit_code
}

# == Comparaison ================================================================

# core_compare <ancienne_b3> <nouvelle_b3> <outdir>
#
# Contrat d'entrée :
#   $1 - chemin vers l'ancienne base .b3 (validée par core_assert_b3_valid)
#   $2 - chemin vers la nouvelle base .b3 (validée par core_assert_b3_valid)
#   $3 - dossier de sortie (doit exister avant l'appel)
#
# Contrat de sortie :
#   exit 0  - comparaison effectuée (même si des différences existent)
#   exit 1  - erreur technique (b3sum, awk, join, comm)
#   $3/modifies.b3   - fichiers présents dans les deux bases avec hashes différents
#                      Format : "<nouveau_hash>  <chemin>" (format b3sum)
#   $3/disparus.txt  - chemins présents dans $1, absents de $2 (un chemin par ligne)
#   $3/nouveaux.txt  - chemins absents de $1, présents dans $2 (un chemin par ligne)
#   CORE_COMPARE_NB_MOD  - entier : nombre de fichiers modifiés
#   CORE_COMPARE_NB_DIS  - entier : nombre de fichiers disparus
#   CORE_COMPARE_NB_NOU  - entier : nombre de nouveaux fichiers
#
# Algorithme :
#   1. Conversion "<hash>  <chemin>" → "<chemin>\t<hash>" via awk (offset fixe 64+2)
#      Robuste aux chemins avec espaces : le séparateur est le tab, pas l'espace
#   2. sort par chemin (clé 1 uniquement)
#   3. join inner sur le chemin → identifie les modifiés (hashes différents)
#   4. comm -23 / -13 sur les chemins → disparus et nouveaux
#
# Effets de bord :
#   - Écrit $3/modifies.b3, $3/disparus.txt, $3/nouveaux.txt
#   - Positionne CORE_COMPARE_NB_* dans le scope de l'appelant
#   - Utilise mktemp pour les fichiers temporaires (nettoyés via trap EXIT)
core_compare() {
  local old="$1"
  local new="$2"
  local outdir="$3"

  local tmp_old tmp_new
  tmp_old=$(mktemp)
  tmp_new=$(mktemp)

  trap 'rm -f "$tmp_old" "$tmp_new"' EXIT

  # Conversion vers format "chemin\thash" - offset fixe 64 chars pour le hash
  # Robuste aux espaces dans les chemins
  _b3_to_path_hash() {
    awk '{ print substr($0,67) "\t" substr($0,1,64) }' "$1" | sort -t $'\t' -k1,1
  }

  _b3_to_path_hash "$old" > "$tmp_old"
  _b3_to_path_hash "$new" > "$tmp_new"

  # Fichiers modifiés : présents dans les deux bases, hashes différents
  join -t $'\t' -1 1 -2 1 "$tmp_old" "$tmp_new" \
    | awk -F $'\t' '$2 != $3 { print $3 "  " $1 }' \
    > "${outdir}/modifies.b3"

  # Fichiers disparus : dans old, pas dans new
  comm -23 <(cut -f1 "$tmp_old") <(cut -f1 "$tmp_new") > "${outdir}/disparus.txt"

  # Nouveaux fichiers : dans new, pas dans old
  comm -13 <(cut -f1 "$tmp_old") <(cut -f1 "$tmp_new") > "${outdir}/nouveaux.txt"

  # Variables de sortie lues par l'appelant (integrity.sh)
  # shellcheck disable=SC2034
  CORE_COMPARE_NB_MOD=$(wc -l < "${outdir}/modifies.b3")
  # shellcheck disable=SC2034
  CORE_COMPARE_NB_DIS=$(wc -l < "${outdir}/disparus.txt")
  # shellcheck disable=SC2034
  CORE_COMPARE_NB_NOU=$(wc -l < "${outdir}/nouveaux.txt")

  rm -f "$tmp_old" "$tmp_new"
  trap - EXIT
}

# == Gestion des dossiers de résultats ==========================================

# core_make_result_dir <fichier_b3> <resultats_dir>
#
# Contrat d'entrée :
#   $1 - chemin vers le fichier .b3 (utilisé pour nommer le dossier)
#   $2 - dossier racine des résultats (RESULTATS_DIR)
#
# Contrat de sortie :
#   stdout - chemin absolu du dossier de résultats créé
#   exit 0 - dossier créé avec succès
#   exit 1 - échec de création (permissions, chemin invalide)
#
# Invariant anti-écrasement :
#   Si "<resultats_dir>/resultats_<nom_base>" existe déjà, un suffixe horodaté
#   "_YYYYMMDD-HHMMSS" est ajouté. Aucun résultat existant n'est jamais écrasé.
core_make_result_dir() {
  local b3file="$1"
  local resultats_dir="$2"

  local basename_noext
  basename_noext=$(basename "$b3file" .b3)
  local outdir="${resultats_dir}/resultats_${basename_noext}"

  if [ -d "$outdir" ]; then
    outdir="${outdir}_$(date +%Y%m%d-%H%M%S)"
  fi

  mkdir -p "$outdir" || die "Impossible de créer le dossier de résultats : $outdir"
  echo "$outdir"
}

# == Sidecar file ===============================================================

# core_sidecar_write <b3_path> <data_dir> <comment> <version>
#
# Génère un fichier <b3_path>.meta.json contenant les métadonnées du compute.
#
# Contrat d'entrée :
#   $1 - chemin du fichier .b3 produit (utilisé pour nommer le sidecar)
#   $2 - dossier source ayant été haché
#   $3 - commentaire libre (peut être vide)
#   $4 - version de l'outil (ex. "integrity.sh v2.0.0" ou "hash-tool v2.0.0")
#
# Contrat de sortie :
#   exit 0  - sidecar créé : <b3_path>.meta.json
#   exit 1  - jq introuvable (silencieux : pas d'erreur fatale, compute reste valide)
#   stdout  - aucun
#
# Invariants :
#   - Le fichier .b3 doit exister avant l'appel (nb_files lu via wc -l)
#   - Si jq est absent, la fonction retourne silencieusement sans créer le sidecar
#   - Le sidecar n'écrase pas un éventuel sidecar existant : c'est un nouveau compute
#
# Effets de bord :
#   - Écrit <b3_path>.meta.json sur le disque
core_sidecar_write() {
  local b3_path="$1"
  local data_dir="$2"
  local comment="${3:-}"
  local version="${4:-integrity.sh}"
  local sidecar_path="${b3_path}.meta.json"

  # jq requis - absence non fatale
  command -v jq &>/dev/null || return 0

  local nb_files
  nb_files=$(wc -l < "$b3_path" 2>/dev/null || echo 0)

  jq -n \
    --arg version  "$version" \
    --arg date     "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
    --arg comment  "$comment" \
    --arg dir      "$data_dir" \
    --argjson nb   "$nb_files" \
    '{
      created_by: $version,
      date:       $date,
      comment:    $comment,
      parameters: {
        directory:  $dir,
        hash_algo:  "blake3",
        nb_files:   $nb
      }
    }' > "$sidecar_path"
}

# core_sidecar_read <b3_path>
#
# Affiche le contenu du sidecar associé à un fichier .b3 si celui-ci existe.
# Aucun effet si le sidecar est absent ou si jq est indisponible.
#
# Contrat d'entrée :
#   $1 - chemin du fichier .b3 (le sidecar est <b3_path>.meta.json)
#
# Contrat de sortie :
#   exit 0  - toujours
#   stdout  - contenu JSON formaté si sidecar présent ; rien sinon
#
# Effets de bord : aucun
core_sidecar_read() {
  local b3_path="$1"
  local sidecar_path="${b3_path}.meta.json"

  [ -f "$sidecar_path" ] || return 0

  echo "--- Métadonnées (sidecar) ---"
  if command -v jq &>/dev/null; then
    jq '.' "$sidecar_path" 2>/dev/null || cat "$sidecar_path"
  else
    cat "$sidecar_path"
  fi
  echo "-----------------------------"
}

--- Fichier : src/lib/report.sh ---
#!/usr/bin/env bash
# lib/report.sh - Génération des rapports de résultats à partir d'un template
#
# Sourcé par integrity.sh. Ne pas exécuter directement.

# _html_escape <string>
# Échappe les caractères HTML de base.
_html_escape() {
  local s="$1"
  s="${s//&/&amp;}"
  s="${s//</&lt;}"
  s="${s//>/&gt;}"
  printf '%s' "$s"
}

# _render_html_file_list <fichier_source> <message_si_vide>
_render_html_file_list() {
  local file="$1"
  local empty_msg="$2"

  if [ ! -s "$file" ]; then
    printf '<p class="empty-msg">%s</p>\n' "$(_html_escape "$empty_msg")"
    return
  fi

  echo "<ul class=\"file-list\">"
  # shellcheck disable=SC2094
    while IFS= read -r line; do
      [ -n "$line" ] || continue
      local display
      if [[ "$file" == *.b3 ]]; then
        display=$(echo "$line" | awk '{ $1=""; print substr($0,2) }' | sed 's/^[ ]*//')
      else
        display="$line"
      fi
      printf '  <li>%s</li>\n' "$(_html_escape "$display")"
    done < "$file"
  echo "</ul>"
}

# generate_compare_html
generate_compare_html() {
  local old_b3="$1"
  local new_b3="$2"
  local nb_modifies="$3"
  local nb_disparus="$4"
  local nb_nouveaux="$5"
  local modifies_file="$6"
  local disparus_file="$7"
  local nouveaux_file="$8"
  local output_html="$9"

  local template_path
  template_path="${SCRIPT_DIR}/../reports/template.html"

  if [ ! -f "$template_path" ]; then
    die "Template de rapport introuvable : $template_path"
  fi

  local date_rapport
  date_rapport=$(date '+%Y-%m-%d %H:%M:%S')

  local nom_old nom_new
  nom_old=$(basename "$old_b3")
  nom_new=$(basename "$new_b3")

  local title="Rapport de comparaison : ${nom_old} vs ${nom_new}"
  local paths="Base de référence : <code>${nom_old}</code> &nbsp;&middot;&nbsp; Base comparée : <code>${nom_new}</code>"

  local status_text status_color
  if (( nb_modifies == 0 && nb_disparus == 0 && nb_nouveaux == 0 )); then
    status_text="IDENTIQUES"
    status_color="var(--accent-ok)"
  else
    status_text="DIFFÉRENCES"
    status_color="var(--accent-err)"
  fi

  # Génération des blocs HTML dans des fichiers temporaires
  local tmp_modified tmp_deleted tmp_new tmp_meta
  tmp_modified=$(mktemp)
  tmp_deleted=$(mktemp)
  tmp_new=$(mktemp)
  tmp_meta=$(mktemp)
  # shellcheck disable=SC2064
  trap "rm -f '$tmp_modified' '$tmp_deleted' '$tmp_new' '$tmp_meta'" RETURN

  _render_html_file_list "$modifies_file" "Aucun fichier modifié."  > "$tmp_modified"
  _render_html_file_list "$disparus_file" "Aucun fichier disparu."  > "$tmp_deleted"
  _render_html_file_list "$nouveaux_file" "Aucun nouveau fichier."  > "$tmp_new"
  printf '<div class="info-label">Métadonnées</div><div class="info-value">Non implémenté</div>\n' > "$tmp_meta"

  # Injection via awk : les scalaires via -v, les blocs multilignes via ENVIRON + fichiers
  TITLE="$title" \
  PATHS="$paths" \
  STATUS_TEXT="$status_text" \
  STATUS_COLOR="$status_color" \
  DATE_RAPPORT="$date_rapport" \
  TMP_META="$tmp_meta" \
  TMP_MODIFIED="$tmp_modified" \
  TMP_DELETED="$tmp_deleted" \
  TMP_NEW="$tmp_new" \
  awk '
  function slurp(path,    line, buf) {
    buf = ""
    while ((getline line < path) > 0) buf = buf line "\n"
    close(path)
    return buf
  }
  BEGIN {
    list_modified = slurp(ENVIRON["TMP_MODIFIED"])
    list_deleted  = slurp(ENVIRON["TMP_DELETED"])
    list_new      = slurp(ENVIRON["TMP_NEW"])
    metadata_rows = slurp(ENVIRON["TMP_META"])
    # Supprimer le newline final pour éviter les lignes vides dans le HTML
    sub(/\n$/, "", list_modified)
    sub(/\n$/, "", list_deleted)
    sub(/\n$/, "", list_new)
    sub(/\n$/, "", metadata_rows)
  }
  {
    gsub("{{TITLE}}",         ENVIRON["TITLE"])
    gsub("{{PATHS}}",         ENVIRON["PATHS"])
    gsub("{{STATUS_TEXT}}",   ENVIRON["STATUS_TEXT"])
    gsub("{{STATUS_COLOR}}",  ENVIRON["STATUS_COLOR"])
    gsub("{{DATE}}",          ENVIRON["DATE_RAPPORT"])
    gsub("{{METADATA_ROWS}}", metadata_rows)
    gsub("{{LIST_MODIFIED}}", list_modified)
    gsub("{{LIST_DELETED}}",  list_deleted)
    gsub("{{LIST_NEW}}",      list_new)
    print
  }
  ' "$template_path" > "$output_html"
}

--- Fichier : src/lib/results.sh ---
#!/usr/bin/env bash
# src/lib/results.sh - Écriture des fichiers de résultats texte
#
# Ce module produit les fichiers recap.txt et failed.txt à partir des
# données de sortie de core_verify() et core_compare().
# Il ne contient ni logique métier ni logique d'affichage terminal.
#
# Sourcé par src/integrity.sh. Ne pas exécuter directement.

# == Résultats de vérification ==================================================

# results_write_verify <outdir> <hashfile> <statut> <nb_ok> <nb_fail> <lines_fail> <lines_err>
#
# Contrat d'entrée :
#   $1 - dossier de sortie (doit exister)
#   $2 - chemin du fichier .b3 vérifié
#   $3 - statut : "OK" | "ECHEC" | "ERREUR"
#   $4 - nombre de fichiers OK
#   $5 - nombre de fichiers FAILED
#   $6 - lignes FAILED (chaîne multi-lignes, peut être vide)
#   $7 - lignes d'erreur b3sum (chaîne multi-lignes, peut être vide)
#
# Contrat de sortie :
#   exit 0
#   $1/recap.txt  - toujours créé
#   $1/failed.txt - créé si $5 > 0 ou $7 non vide ; supprimé sinon
#                   (suppression pour éviter un failed.txt obsolète d'un run précédent)
#
# Effets de bord : écrit sur le disque
results_write_verify() {
  local outdir="$1"
  local hashfile="$2"
  local statut="$3"
  local nb_ok="$4"
  local nb_fail="$5"
  local lines_fail="$6"
  local lines_err="$7"

  # recap.txt
  {
    echo "========================================"
    echo "  STATUT : $statut"
    echo "========================================"
    echo ""
    echo "Commande  : integrity.sh verify $(basename "$hashfile")"
    echo "Date      : $(date)"
    echo "Base      : $hashfile"
    echo ""
    echo "OK        : $nb_ok"
    if (( nb_fail > 0 )); then
      echo "FAILED    : $nb_fail  ← voir failed.txt"
    fi
    if [ -n "$lines_err" ]; then
      echo ""
      echo "== Erreurs b3sum ======================"
      echo "$lines_err"
    fi
  } > "${outdir}/recap.txt"

  # failed.txt
  if (( nb_fail > 0 )) || [ -n "$lines_err" ]; then
    {
      echo "========================================"
      echo "  FICHIERS EN ECHEC"
      echo "========================================"
      echo ""
      [ -n "$lines_fail" ] && echo "$lines_fail"
      if [ -n "$lines_err" ]; then
        echo ""
        echo "== Erreurs ============================"
        echo "$lines_err"
      fi
    } > "${outdir}/failed.txt"
  else
    rm -f "${outdir}/failed.txt"
  fi
}

# == Résultats de comparaison ===================================================

# results_write_compare <outdir> <old_b3> <new_b3> <nb_mod> <nb_dis> <nb_nou>
#
# Contrat d'entrée :
#   $1 - dossier de sortie (doit exister, contient déjà modifies.b3, disparus.txt, nouveaux.txt)
#   $2 - chemin de l'ancienne base .b3
#   $3 - chemin de la nouvelle base .b3
#   $4 - nombre de fichiers modifiés
#   $5 - nombre de fichiers disparus
#   $6 - nombre de nouveaux fichiers
#
# Contrat de sortie :
#   exit 0
#   $1/recap.txt - créé
#
# Effets de bord : écrit sur le disque
results_write_compare() {
  local outdir="$1"
  local old="$2"
  local new="$3"
  local nb_mod="$4"
  local nb_dis="$5"
  local nb_nou="$6"

  {
    echo "Commande      : integrity.sh compare $(basename "$old") $(basename "$new")"
    echo "Date          : $(date)"
    echo "Ancienne base : $old"
    echo "Nouvelle base : $new"
    echo ""
    echo "Modifiés      : $nb_mod"
    echo "Disparus      : $nb_dis"
    echo "Nouveaux      : $nb_nou"
  } > "${outdir}/recap.txt"
}


--- Fichier : src/lib/ui.sh ---
#!/usr/bin/env bash
# src/lib/ui.sh - Logique d'interface : affichage terminal, ETA, progression
#
# Ce module contient uniquement la logique de présentation et d'interaction
# avec l'utilisateur. Il ne contient aucune logique métier.
#
# Sourcé par src/integrity.sh. Ne pas exécuter directement.
#
# == Dépendances ================================================================
#   Aucune dépendance externe. Utilise uniquement les builtins bash et printf.
#
# == Prérequis ==================================================================
#   La variable QUIET doit être définie avant de sourcer ce module :
#     QUIET=0  - affichage normal
#     QUIET=1  - suppression de toute sortie terminal

# == Primitives de communication ================================================

# die <message>
#
# Contrat d'entrée :
#   $@ - message d'erreur (chaîne quelconque)
#
# Contrat de sortie :
#   exit 1 - toujours
#   stderr - "ERREUR : <message>"
#
# Effets de bord : termine le processus courant
die() {
  echo "ERREUR : $*" >&2
  exit 1
}

# say <message>
#
# Contrat d'entrée :
#   $@ - message à afficher (chaîne quelconque)
#
# Contrat de sortie :
#   stdout - <message> si QUIET == 0
#   (rien)  - si QUIET == 1
#
# Effets de bord : aucun
say() {
  (( QUIET )) || echo "$@"
}

# == Progression et ETA =========================================================

# ui_progress_callback <i> <total_files> <bytes_done> <total_bytes> <eta_seconds>
#
# Fonction de callback compatible avec core_compute().
# À passer comme troisième argument de core_compute si la progression est souhaitée.
#
# Contrat d'entrée :
#   $1 - index du fichier courant (entier, commence à 1)
#   $2 - nombre total de fichiers
#   $3 - octets traités jusqu'ici
#   $4 - octets totaux
#   $5 - ETA en secondes (0 si non calculable)
#
# Contrat de sortie :
#   /dev/tty - ligne de progression sur le terminal courant
#              Écrit sur /dev/tty et non sur stdout : garantit que la progression
#              ne peut pas être capturée dans un pipe ou dans le fichier .b3
#   (rien)   - si QUIET == 1
#
# Effets de bord : aucun (le \r efface la ligne précédente)
ui_progress_callback() {
  (( QUIET )) && return 0

  local i="$1"
  local total_files="$2"
  local bytes_done="$3"
  # shellcheck disable=SC2034  # total_bytes : reçu du callback mais non utilisé ici (réservé pour affichage %)
  local total_bytes="$4"
  local eta_seconds="$5"

  if (( bytes_done > 0 && eta_seconds > 0 )); then
    printf "\r[%d/%d] ETA : %dm %02ds   " \
      "$i" "$total_files" $(( eta_seconds / 60 )) $(( eta_seconds % 60 )) > /dev/tty
  elif (( bytes_done > 0 )); then
    printf "\r[%d/%d] calcul en cours...   " \
      "$i" "$total_files" > /dev/tty
  fi
}

# ui_progress_clear
#
# Efface la ligne de progression ETA du terminal.
# À appeler après core_compute pour laisser un terminal propre.
#
# Contrat de sortie :
#   /dev/tty - ligne vide (40 espaces + \r)
#   (rien)   - si QUIET == 1
ui_progress_clear() {
  (( QUIET )) && return 0
  printf "\r%*s\r" 40 "" > /dev/tty
}

# == Affichage des résultats de vérification ====================================

# ui_show_verify_result <statut> <nb_ok> <nb_fail> <lines_fail> <lines_err> <outdir>
#
# Contrat d'entrée :
#   $1 - statut : "OK" | "ECHEC" | "ERREUR"
#   $2 - nombre de fichiers OK (entier)
#   $3 - nombre de fichiers FAILED (entier)
#   $4 - lignes FAILED (chaîne multi-lignes, peut être vide)
#   $5 - lignes d'erreur b3sum (chaîne multi-lignes, peut être vide)
#   $6 - chemin du dossier de résultats
#
# Contrat de sortie :
#   stdout - résumé formaté si QUIET == 0
#   (rien)  - si QUIET == 1
#
# Effets de bord : aucun
ui_show_verify_result() {
  local statut="$1"
  local nb_ok="$2"
  local nb_fail="$3"
  local lines_fail="$4"
  local lines_err="$5"
  local outdir="$6"

  if [ "$statut" = "OK" ]; then
    say "Vérification OK - $nb_ok fichiers intègres."
  else
    say ""
    say "████████████████████████████████████████"
    if [ "$statut" = "ERREUR" ]; then
      say "  ERREUR lors de la vérification"
    else
      say "  ECHEC : $nb_fail fichier(s) corrompu(s) ou manquant(s)"
    fi
    say "████████████████████████████████████████"
    say ""
    [ -n "$lines_fail" ] && say "$lines_fail"
    [ -n "$lines_err"  ] && say "$lines_err"
    say ""
  fi

  say "Résultats dans : $outdir"
  say "  recap.txt"
  (( nb_fail > 0 )) || [ -n "$lines_err" ] && say "  failed.txt"
}

# ui_show_compare_result <nb_mod> <nb_dis> <nb_nou> <outdir>
#
# Contrat d'entrée :
#   $1 - nombre de fichiers modifiés
#   $2 - nombre de fichiers disparus
#   $3 - nombre de nouveaux fichiers
#   $4 - chemin du dossier de résultats
#
# Contrat de sortie :
#   stdout - résumé formaté si QUIET == 0
#   (rien)  - si QUIET == 1
#
# Effets de bord : aucun
ui_show_compare_result() {
  local nb_mod="$1"
  local nb_dis="$2"
  local nb_nou="$3"
  local outdir="$4"

  say "Résultats enregistrés dans : $outdir"
  say "  recap.txt     - modifiés: $nb_mod, disparus: $nb_dis, nouveaux: $nb_nou"
  say "  modifies.b3   - $nb_mod fichiers"
  say "  disparus.txt  - $nb_dis fichiers"
  say "  nouveaux.txt  - $nb_nou fichiers"
  say "  report.html   - rapport visuel"
}

--- Fichier : docker/entrypoint.sh ---
#!/usr/bin/env bash
# /entrypoint.sh - Point d'entrée Docker pour hash_tool
#
# Dispatche les commandes vers integrity.sh ou runner.sh.
# Toutes les commandes de integrity.sh sont supportées directement.
#
# Exemples :
#   docker run hash_tool help
#   docker run hash_tool compute /data /bases/hashes.b3
#   docker run hash_tool verify  /bases/hashes.b3
#   docker run hash_tool compare /bases/old.b3 /bases/new.b3
#   docker run hash_tool runner  /pipelines/pipeline.json
#   docker run hash_tool runner                              # lit /pipelines/pipeline.json
#   docker run -it hash_tool shell                           # bash interactif (debug)

set -euo pipefail

APP="/app"
INTEGRITY="$APP/src/integrity.sh"
RUNNER="$APP/runner.sh"

# == Aide =====================================================================

print_help() {
  cat <<'EOF'
hash_tool - Vérification d'intégrité BLAKE3

Usage :
  docker run [--rm] [-v ...] hash_tool <commande> [arguments...]

Commandes :
  compute <dossier> <base.b3>       Calcule les hashes d'un dossier
  verify  <base.b3> [dossier]       Vérifie l'intégrité
  compare <ancienne.b3> <nouvelle.b3>  Compare deux bases
  runner  [pipeline.json]           Exécute un pipeline (défaut : /pipelines/pipeline.json)
  shell                             Lance un shell bash interactif (debug)
  help                              Affiche cette aide

Options globales (à placer avant la commande) :
  --quiet                           Supprime la sortie terminal

Volumes conventionnels :
  /data        → données à hacher        (-v /mes/donnees:/data)
  /bases       → fichiers .b3            (-v /mes/bases:/bases)
  /pipelines   → fichiers pipeline.json  (-v /chemin/pipeline.json:/pipelines/pipeline.json)
  /resultats   → résultats               (-v /mes/resultats:/resultats)

Variable d'environnement :
  RESULTATS_DIR  Dossier de résultats (défaut dans le conteneur : /resultats)

Exemples :
  # Calculer les hashes de /data, stocker dans /bases
  docker run --rm \
    -v /mes/donnees:/data:ro \
    -v /mes/bases:/bases \
    hash_tool compute /data /bases/hashes_$(date +%Y-%m-%d).b3

  # Vérifier depuis le dossier d'origine
  docker run --rm \
    -v /mes/donnees:/data:ro \
    -v /mes/bases:/bases:ro \
    -v /mes/resultats:/resultats \
    hash_tool verify /bases/hashes_2024-01-15.b3 /data

  # Comparer deux snapshots
  docker run --rm \
    -v /mes/bases:/bases:ro \
    -v /mes/resultats:/resultats \
    hash_tool compare /bases/hashes_2024-01-15.b3 /bases/hashes_2024-02-01.b3

  # Pipeline complet depuis un fichier JSON
  docker run --rm \
    -v /mes/donnees:/data:ro \
    -v /mes/bases:/bases \
    -v /mes/resultats:/resultats \
    -v /chemin/vers/pipeline.json:/pipelines/pipeline.json:ro \
    hash_tool runner

  # Mode silencieux (CI/cron)
  docker run --rm \
    -v /mes/donnees:/data:ro \
    -v /mes/bases:/bases:ro \
    -v /mes/resultats:/resultats \
    hash_tool --quiet verify /bases/hashes.b3 /data

EOF
}

# == Vérification des outils ===================================================

check_deps() {
  local ok=1
  command -v b3sum &>/dev/null || { echo "ERREUR : b3sum introuvable" >&2; ok=0; }
  command -v jq    &>/dev/null || { echo "ERREUR : jq introuvable"    >&2; ok=0; }
  [ -f "$INTEGRITY" ]          || { echo "ERREUR : $INTEGRITY introuvable" >&2; ok=0; }
  [ -f "$RUNNER" ]             || { echo "ERREUR : $RUNNER introuvable"    >&2; ok=0; }
  (( ok )) || exit 1
}

# == Dispatch ==================================================================

# Extraire --quiet en tête s'il est présent
QUIET_FLAG=""
if [ "${1:-}" = "--quiet" ]; then
  QUIET_FLAG="--quiet"
  shift
fi

CMD="${1:-help}"
shift || true

case "$CMD" in

  compute|verify|compare)
    check_deps
    exec bash "$INTEGRITY" $QUIET_FLAG "$CMD" "$@"
    ;;

  runner)
    check_deps
    PIPELINE="${1:-/pipelines/pipeline.json}"
    if [ ! -f "$PIPELINE" ]; then
      echo "ERREUR : pipeline.json introuvable : $PIPELINE" >&2
      echo "Monter le fichier avec : -v /chemin/pipeline.json:/pipelines/pipeline.json" >&2
      exit 1
    fi
    exec bash "$RUNNER" "$PIPELINE"
    ;;

  shell|bash)
    echo "hash_tool - shell interactif (debug)"
    echo "  b3sum    : $(b3sum --version 2>/dev/null || echo 'non trouvé')"
    echo "  jq       : $(jq --version 2>/dev/null || echo 'non trouvé')"
    echo "  bash     : $BASH_VERSION"
    echo ""
    exec /bin/bash
    ;;

  help|--help|-h)
    print_help
    ;;

  version|--version|-v)
    echo "hash_tool"
    echo "  b3sum : $(b3sum --version 2>/dev/null || echo 'non trouvé')"
    echo "  jq    : $(jq --version 2>/dev/null || echo 'non trouvé')"
    echo "  bash  : $BASH_VERSION"
    ;;

  *)
    echo "ERREUR : commande inconnue : '$CMD'" >&2
    echo "Lancer 'docker run hash_tool help' pour la liste des commandes." >&2
    exit 1
    ;;

esac


--- Fichier : pipelines/pipeline-amelioree.json ---
{
  "pipeline": [
    {
      "type": "compute",
      "params": {
        "input":      "/mnt/data/dossier_exemple",
        "output_dir": "/mnt/bases",
        "filename":   "hashes_exemple.b3"
      },
      "options": {
        "quiet":    false,
        "verbose":  false,
        "readonly": true
      },
      "meta": {
        "comment": "Snapshot initial avant migration"
      },
      "description": "Calculer les empreintes du dossier source (lecture seule)"
    },
    {
      "type": "verify",
      "params": {
        "input": "/mnt/data/dossier_exemple",
        "base":  "/mnt/bases/hashes_exemple.b3"
      },
      "options": {
        "quiet": false
      },
      "description": "Vérifier l'intégrité immédiatement après compute"
    },
    {
      "type": "compute",
      "params": {
        "input":      "/mnt/data/dossier_exemple_v2",
        "output_dir": "/mnt/bases",
        "filename":   "hashes_exemple_v2.b3"
      },
      "options": {
        "quiet":    false,
        "readonly": true
      },
      "meta": {
        "comment": "Snapshot après migration"
      },
      "description": "Calculer les empreintes du dossier destination (post-migration)"
    },
    {
      "type": "compare",
      "params": {
        "reference":  "/mnt/bases/hashes_exemple.b3",
        "input":      "/mnt/bases/hashes_exemple_v2.b3",
        "output_dir": "/mnt/resultats/compare_migration"
      },
      "options": {
        "quiet": false
      },
      "description": "Comparer avant/après migration et produire le rapport HTML"
    },
    {
      "type": "list",
      "params": {
        "input_dir": "/mnt/bases"
      },
      "options": {},
      "description": "Lister les bases d'empreintes disponibles"
    },
    {
      "type": "diff",
      "params": {
        "input":         "/mnt/bases/hashes_exemple.b3",
        "reference_dir": "/mnt/data/dossier_exemple"
      },
      "options": {},
      "description": "Afficher les différences entre la base et le dossier"
    },
    {
      "type": "stats",
      "params": {
        "input": "/mnt/bases/hashes_exemple.b3"
      },
      "options": {},
      "description": "Afficher les statistiques de la base d'empreintes"
    },
    {
      "type": "check-env",
      "params": {},
      "options": {},
      "description": "Vérifier si l'exécution native ou Docker est possible"
    },
    {
      "type": "version",
      "params": {},
      "options": {},
      "description": "Afficher la version du logiciel"
    }
  ]
}


--- Fichier : pipelines/pipeline-debug-deux-adresses.json ---
{
    "pipeline": [

        {
            "op":     "compute",
            "source": "./mon_dossier/source",
            "bases":  "./mon_dossier/bases",
            "nom":    "hashes_dossier_1.b3"
        },

        {
            "op":     "compute",
            "source": "/home/me-dell/Bureau/destination",
            "bases":  "/home/me-dell/Bureau/dossier bureau/bases",
            "nom":    "hashes_dossier_2.b3"
        },

        {
            "op":       "compare",
            "base_a":   "./mon_dossier/bases/hashes_dossier_1.b3",
            "base_b":   "/home/me-dell/Bureau/dossier bureau/bases/bases/hashes_dossier_2.b3",
            "resultats": "./mon_dossier/result"
        }

    ]
}

--- Fichier : pipelines/pipeline-debug.json ---
{
    "pipeline": [

        {
            "op":     "compute",
            "source": "./mon_dossier/source",
            "bases":  "./mon_dossier/bases",
            "nom":    "hashes_dossier_1.b3"
        },

        {
            "op":     "compute",
            "source": "./mon_dossier/destination",
            "bases":  "./mon_dossier/bases",
            "nom":    "hashes_dossier_2.b3"
        },

        {
            "op":       "compare",
            "base_a":   "./mon_dossier/bases/hashes_dossier_1.b3",
            "base_b":   "./mon_dossier/bases/hashes_dossier_2.b3",
            "resultats": "./mon_dossier/result"
        }

    ]
}

--- Fichier : pipelines/pipeline-veracrypt.json ---
{
    "pipeline": [

        {
            "op":     "compute",
            "source": "/mnt/a/dossier_disque_1",
            "bases":  "/mnt/c/Users/TonNom/Desktop/bases",
            "nom":    "hashes_dossier_1.b3"
        },

        {
            "op":     "compute",
            "source": "/mnt/i/dossier_disque_2",
            "bases":  "/mnt/c/Users/TonNom/Desktop/bases",
            "nom":    "hashes_dossier_2.b3"
        },

        {
            "op":     "compute",
            "source": "/mnt/h/dossier_disque_3",
            "bases":  "/mnt/c/Users/TonNom/Desktop/bases",
            "nom":    "hashes_dossier_3.b3"
        },

        {
            "op":     "verify",
            "source": "/mnt/a/dossier_disque_1",
            "base":   "/mnt/c/Users/TonNom/Desktop/bases/hashes_dossier_1.b3"
        },

        {
            "op":       "compare",
            "base_a":   "/mnt/c/Users/TonNom/Desktop/bases/hashes_dossier_1.b3",
            "base_b":   "/mnt/c/Users/TonNom/Desktop/bases/hashes_dossier_2.b3",
            "resultats": "/mnt/c/Users/TonNom/Desktop/rapports/compare_1_vs_2"
        }

    ]
}

--- Fichier : pipelines/pipeline.json ---
{
    "pipeline": [

        {
            "op":     "compute",
            "source": "./mon_dossier/source",
            "bases":  "./mon_dossier/bases",
            "nom":    "hashes_dossier_1.b3"
        },

        {
            "op":     "compute",
            "source": "./mon_dossier/destination",
            "bases":  "./mon_dossier/bases",
            "nom":    "hashes_dossier_2.b3"
        },

        {
            "op":       "compare",
            "base_a":   "./mon_dossier/bases/hashes_dossier_1.b3",
            "base_b":   "./mon_dossier/bases/hashes_dossier_2.b3",
            "resultats": "./mon_dossier/result"
        }

    ]
}

